{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gothic-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "understood-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6192\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/costa/radardata/training_data/\"\n",
    "\n",
    "# Get results file paths\n",
    "npyfiles = [f for f in glob.glob(data_path + \"*.plk\")]\n",
    "npyfiles.sort()\n",
    "print(len(npyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alien-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random select four years as test dataset  \n",
    "#test_yr = np.random.choice(np.arange(2001,2018),4,replace=False) #2009 2003 2014 2016\n",
    "#test_yr = [2003,2009,2014,2016]\n",
    "test_yr = [2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attempted-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2005]\n",
      "364 1457\n"
     ]
    }
   ],
   "source": [
    "print(test_yr)\n",
    "train_data_path  = []\n",
    "test_data_path = []\n",
    "\n",
    "for yr in range(2001,2006):\n",
    "#for yr in range(2001,2018):\n",
    "    npyfiles = [f for f in glob.glob(data_path +\"train_\"+str(yr)+\"*.plk\")]\n",
    "    \n",
    "    if yr in test_yr:\n",
    "        test_data_path.extend(npyfiles)\n",
    "    else:\n",
    "        train_data_path.extend(npyfiles)\n",
    "print(len(test_data_path),len(train_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "executive-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_df_prep(train_data_path,save_df=False):\n",
    "    columns_names = ['track','nearest_pts']\n",
    "    output_df = pd.DataFrame(columns=columns_names)\n",
    "    for df_path in train_data_path:\n",
    "        df = pd.read_pickle(df_path)\n",
    "        output_df = output_df.append(df[['track','nearest_pts']] ,ignore_index=True)\n",
    "    if save_df:\n",
    "        output_df.to_pickle('train_df_tmp.plk')\n",
    "    return output_df\n",
    "    \n",
    "    \n",
    "def test_df_prep(test_data_path,save_df=False):\n",
    "    columns_names = ['track','nearest_pts','LK_lin4','LK_lin4_dist']\n",
    "    output_df = pd.DataFrame(columns=columns_names)\n",
    "    for df_path in test_data_path:\n",
    "        df = pd.read_pickle(df_path)\n",
    "        output_df = output_df.append(df[['track','nearest_pts','LK_lin4','LK_lin4_dist']], ignore_index=True)\n",
    "    if save_df:\n",
    "        output_df.to_pickle('test_df_tmp.plk')\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "square-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.90306091308594\n"
     ]
    }
   ],
   "source": [
    "# train data set\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "train_df = train_df_prep(train_data_path)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amber-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_pickle('train_df.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handy-federal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.49457049369812\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "start = time.time()\n",
    "test_df = test_df_prep(test_data_path)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "identified-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation function\n",
    "def filter_dxy(input_train,target_train ,max_value):\n",
    "\n",
    "    msk_in_dx = np.logical_and((input_train[:,:,0]<=max_value).any(axis=1),\n",
    "                               (input_train[:,:,0]>=-max_value).any(axis=1))\n",
    "    msk_in_dy = np.logical_and((input_train[:,:,1]<=max_value).any(axis=1),\n",
    "                               (input_train[:,:,1]>=-max_value).any(axis=1))\n",
    "    \n",
    "    msk_input = np.logical_and(msk_in_dx,msk_in_dy)\n",
    "    \n",
    "    msk_tg_dx = np.logical_and(target_train[:,0]<=max_value,\n",
    "                               target_train[:,0]>=-max_value)\n",
    "    msk_tg_dy = np.logical_and(target_train[:,1]<=max_value,\n",
    "                               target_train[:,1]>=-max_value)\n",
    "    \n",
    "    msk_target = np.logical_and(msk_tg_dx,msk_tg_dy)\n",
    "    \n",
    "    return np.logical_and(msk_input,msk_target)\n",
    "\n",
    "\n",
    "def format_input(input_df):\n",
    "    input_pts = input_df['nearest_pts'].values\n",
    "    input_pts = np.array(input_pts.tolist())\n",
    "    \n",
    "    # Create a new column with the target point lead time\n",
    "    input_df['target_30'] = input_df['track'].apply(lambda row: row[11,:])\n",
    "    # as the forecast time = 30min(time step 5) ,for 30min lead time time step ==11\n",
    "    \n",
    "    target_pts = input_df['target_30'].values\n",
    "    target_pts = np.array(target_pts.tolist())\n",
    "    \n",
    "    #mask displacement greater then 12\n",
    "    mask = filter_dxy(input_pts,target_pts ,max_value=12)\n",
    "    \n",
    "    inp_shape = input_pts.shape[0]\n",
    "    trg_shape = target_pts.shape[0]\n",
    "    \n",
    "    input_pts = input_pts[mask]\n",
    "    target_pts = target_pts[mask]\n",
    "    \n",
    "    print(\"filtered input\",inp_shape-input_pts.shape[0])\n",
    "    print(\"filtered target\",trg_shape-target_pts.shape[0])\n",
    "    \n",
    "    return input_pts, target_pts\n",
    "\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "    df = pd.read_pickle(file_path)\n",
    "    input_pts, target_pts = format_input(df)\n",
    "    \n",
    "    return input_pts, target_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "racial-wound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered input 534\n",
      "filtered target 534\n"
     ]
    }
   ],
   "source": [
    "input_pts,target_pts = format_input(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mysterious-school",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 70.50601196289062\n",
      "1 94.9197998046875\n",
      "2 861.1214599609375\n",
      "3 1060.1002197265625\n",
      "4 367787.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i,input_pts[:,:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "popular-single",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD4CAYAAADy46FuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZPElEQVR4nO3df5BdZZ3n8fdnibLsuIkBGk0lcTtCVH45ccjEbFlazGYmybiUwS1Ym9qF7E62IhRaWuvWTqJ/kMJKlcwsZocaYSpOMgSWAbKgQ2qVwWxiqVMFgQYzhBAZWnGkTYb0mCymygGr42f/OE/j6eb26U53p29f+Lyqbt1zv+d5Ds9Tgp8+5zn3HtkmIiJiNP+s3QOIiIiZLUERERGNEhQREdEoQREREY0SFBER0WhWuwcw1c4991x3d3e3exgRER3lySef/EfbXa32veGCoru7m97e3nYPIyKio0j6+9H25dJTREQ0SlBERESjBEVERDRKUERERKMERURENEpQREREowRFREQ0SlBERESjBEVERDRKUERERKMERcQp+sr1e7n1E1e0exgR02bMoJC0UNK3JR2SdFDSZ0r9bEm7JT1f3ufW+myU1CfpOUmravXLJB0o+26TpFI/U9L9pb5PUnetz9ryz3he0topnX1ERIxpPGcUg8DnbF8ILAdulHQRsAHYY3sxsKd8puzrAS4GVgO3SzqjHOsOYD2wuLxWl/o64LjtC4AtwC3lWGcDNwEfBJYBN9UDKSIiTr8xg8L2EdtPle0TwCFgPrAG2FGa7QCuLNtrgPtsv2r7BaAPWCZpHjDb9qO2Ddw1os/QsR4AVpSzjVXAbtvHbB8HdvPrcImIiGlwSmsU5ZLQB4B9wDtsH4EqTIDzSrP5wIu1bv2lNr9sj6wP62N7EHgZOKfhWCPHtV5Sr6TegYGBU5lSRESMYdxBIeltwIPAZ23/vKlpi5ob6hPt8+uCvdX2UttLu7paPncjIiImaFxBIektVCFxj+2vlfJL5XIS5f1oqfcDC2vdFwCHS31Bi/qwPpJmAXOAYw3HioiIaTKeu54EbAMO2f5ybdcuYOgupLXAQ7V6T7mTaRHVovXj5fLUCUnLyzGvG9Fn6FhXAXvLOsYjwEpJc8si9spSi4iIaTKeR6F+CLgWOCBpf6l9HvgSsFPSOuAnwNUAtg9K2gk8S3XH1I22T5Z+NwB3AmcBD5cXVEF0t6Q+qjOJnnKsY5K+CDxR2t1s+9jEphoRERMxZlDY/htarxUArBilz2Zgc4t6L3BJi/orlKBpsW87sH2scUZExOmRb2ZHRESjBEVERDRKUERERKMERURENEpQREREowRFREQ0SlBERESjBEXEJFy641IOve/Cdg8j4rRKUERERKMERURENEpQREREowRFREQ0SlBERESjBEVERDRKUERERKMERURENBrPo1C3Szoq6Zla7X5J+8vrx0NPvpPULemfavv+rNbnMkkHJPVJuq08DpXyyNT7S32fpO5an7WSni+vtURExLQbz6NQ7wT+FLhrqGD7E0Pbkm4FXq61/6HtJS2OcwewHngM+CawmupRqOuA47YvkNQD3AJ8QtLZwE3AUsDAk5J22T4+7tlFRMSkjXlGYfu7VM+xfp1yVvDvgXubjiFpHjDb9qO2TRU6V5bda4AdZfsBYEU57ipgt+1jJRx2U4VLRERMo8muUXwYeMn287XaIknfl/QdSR8utflAf61Nf6kN7XsRwPYg1dnJOfV6iz7DSFovqVdS78DAwCSnFBERdZMNimsYfjZxBHiX7Q8A/xX4S0mzAbXo6/I+2r6mPsOL9lbbS20v7erqGvfgIyJibBMOCkmzgH8H3D9Us/2q7Z+V7SeBHwLvoTobWFDrvgA4XLb7gYW1Y86hutT1Wr1Fn4iImCaTOaP4XeAHtl+7pCSpS9IZZfvdwGLgR7aPACckLS/rD9cBD5Vuu4ChO5quAvaWdYxHgJWS5kqaC6wstYiImEZj3vUk6V7gcuBcSf3ATba3AT28fhH7I8DNkgaBk8D1tocWwm+guoPqLKq7nR4u9W3A3ZL6qM4kegBsH5P0ReCJ0u7m2rEiImKajBkUtq8Zpf6fWtQeBB4cpX0vcEmL+ivA1aP02Q5sH2uMERFx+uSb2RER0ShBERERjRIUEePQv+F7bNq0qd3DiGiLBEVERDRKUERERKMERURENEpQREREowRFxCj27D2fd357f7uHEdF2CYqIiGiUoIiIiEYJioiIaJSgiIiIRgmKiIholKCIiIhGCYqIiGiUoIiIiEZjBoWk7ZKOSnqmVtsk6aeS9pfXR2v7Nkrqk/ScpFW1+mWSDpR9t5VHoiLpTEn3l/o+Sd21PmslPV9eQ49LjYiIaTSeM4o7gdUt6ltsLymvbwJIuojqUaYXlz63Dz1DG7gDWE/1HO3FtWOuA47bvgDYAtxSjnU2cBPwQWAZcFN5dnZEREyjMYPC9nepnmU9HmuA+2y/avsFoA9YJmkeMNv2o7YN3AVcWeuzo2w/AKwoZxurgN22j9k+DuymdWBFRMRpNJk1ik9Jerpcmhr6S38+8GKtTX+pzS/bI+vD+tgeBF4Gzmk41utIWi+pV1LvwMDAJKYUEREjTTQo7gDOB5YAR4BbS10t2rqhPtE+w4v2VttLbS/t6upqGHZERJyqCQWF7Zdsn7T9K+CrVGsIUP3Vv7DWdAFwuNQXtKgP6yNpFjCH6lLXaMeKiIhpNKGgKGsOQz4ODN0RtQvoKXcyLaJatH7c9hHghKTlZf3hOuChWp+hO5quAvaWdYxHgJWS5pZLWytLLSIiptGssRpIuhe4HDhXUj/VnUiXS1pCdSnox8AnAWwflLQTeBYYBG60fbIc6gaqO6jOAh4uL4BtwN2S+qjOJHrKsY5J+iLwRGl3s+3xLqpHRMQUGTMobF/Torytof1mYHOLei9wSYv6K8DVoxxrO7B9rDFGRMTpk29mR0REowRFREQ0SlBERESjBEVERDRKUERERKMERURENEpQREREowRFREQ0SlBERESjBEVERDRKUERERKMERURENEpQREREowRFREQ0SlBERESjBEXEFOne8A3YNKfdw4iYcmMGhaTtko5KeqZW+2NJP5D0tKSvS3p7qXdL+idJ+8vrz2p9LpN0QFKfpNvKI1Epj029v9T3Sequ9Vkr6fnyWktEREy78ZxR3AmsHlHbDVxi+/3A3wEba/t+aHtJeV1fq98BrKd6jvbi2jHXAcdtXwBsAW4BkHQ21WNXPwgsA24qz86OiIhpNGZQ2P4u1bOs67Vv2R4sHx8DFjQdQ9I8YLbtR20buAu4suxeA+wo2w8AK8rZxipgt+1jto9ThdPIwIqIiNNsKtYo/gB4uPZ5kaTvS/qOpA+X2nygv9amv9SG9r0IUMLnZeCcer1Fn4iImCazJtNZ0heAQeCeUjoCvMv2zyRdBvyVpIsBtejuocOMsq+pz8hxrKe6rMW73vWu8U8gIiLGNOEzirK4fAXwH8rlJGy/avtnZftJ4IfAe6jOBuqXpxYAh8t2P7CwHHMWMIfqUtdr9RZ9hrG91fZS20u7uromOqWIiGhhQkEhaTXwh8DHbP+iVu+SdEbZfjfVovWPbB8BTkhaXtYfrgMeKt12AUN3NF0F7C3B8wiwUtLcsoi9stQiImIajXnpSdK9wOXAuZL6qe5E2gicCewud7k+Vu5w+ghws6RB4CRwve2hhfAbqO6gOotqTWNoXWMbcLekPqoziR4A28ckfRF4orS7uXasiIiYJmMGhe1rWpS3jdL2QeDBUfb1Ape0qL8CXD1Kn+3A9rHGGBERp0++mR0REY0SFBER0ShBERERjRIUERHRKEERERGNEhQREdEoQREREY0SFBER0ShBERERjRIUERHRKEERERGNEhQREdEoQREREY0SFBER0ShBERERjRIUERHRKEERERGNxgwKSdslHZX0TK12tqTdkp4v73Nr+zZK6pP0nKRVtfplkg6UfbeVZ2cj6UxJ95f6PkndtT5ryz/jeUlDz9WOiIhpNJ4zijuB1SNqG4A9thcDe8pnJF1E9czri0uf2yWdUfrcAawHFpfX0DHXAcdtXwBsAW4pxzqb6vncHwSWATfVAykiIqbHmEFh+7vAsRHlNcCOsr0DuLJWv8/2q7ZfAPqAZZLmAbNtP2rbwF0j+gwd6wFgRTnbWAXstn3M9nFgN68PrIiIOM0mukbxDttHAMr7eaU+H3ix1q6/1OaX7ZH1YX1sDwIvA+c0HOt1JK2X1Cupd2BgYIJTioiIVqZ6MVstam6oT7TP8KK91fZS20u7urrGNdCIiBifiQbFS+VyEuX9aKn3Awtr7RYAh0t9QYv6sD6SZgFzqC51jXasiIiYRhMNil3A0F1Ia4GHavWecifTIqpF68fL5akTkpaX9YfrRvQZOtZVwN6yjvEIsFLS3LKIvbLUIiJiGs0aq4Gke4HLgXMl9VPdifQlYKekdcBPgKsBbB+UtBN4FhgEbrR9shzqBqo7qM4CHi4vgG3A3ZL6qM4kesqxjkn6IvBEaXez7ZGL6hERcZqNGRS2rxll14pR2m8GNreo9wKXtKi/QgmaFvu2A9vHGmNERJw++WZ2REQ0SlBERESjBEVERDRKUERERKMERURENEpQREREowRFREQ0SlBERESjBEVERDRKUERERKMERURENEpQREREowRFREQ0SlBERESjBEVERDRKUERERKMJB4Wk90raX3v9XNJnJW2S9NNa/aO1Phsl9Ul6TtKqWv0ySQfKvtvK41Ipj1S9v9T3Seqe1GwjptE7v72/3UOImBITDgrbz9leYnsJcBnwC+DrZfeWoX22vwkg6SKqx5xeDKwGbpd0Rml/B7Ce6hnbi8t+gHXAcdsXAFuAWyY63ojptGfv+e0eQsSUmapLTyuAH9r++4Y2a4D7bL9q+wWgD1gmaR4w2/ajtg3cBVxZ67OjbD8ArBg624iIiOkxVUHRA9xb+/wpSU9L2i5pbqnNB16stekvtflle2R9WB/bg8DLwDkj/+GS1kvqldQ7MDAwFfOJN6nuDd+ATXPaPYyIGWXSQSHprcDHgP9dSncA5wNLgCPArUNNW3R3Q72pz/CCvdX2UttLu7q6xj/4iIgY01ScUfw+8JTtlwBsv2T7pO1fAV8FlpV2/cDCWr8FwOFSX9CiPqyPpFnAHODYFIw5IiLGaSqC4hpql53KmsOQjwPPlO1dQE+5k2kR1aL147aPACckLS/rD9cBD9X6rC3bVwF7yzpGRERMk1mT6SzpXwC/B3yyVv4jSUuoLhH9eGif7YOSdgLPAoPAjbZPlj43AHcCZwEPlxfANuBuSX1UZxI9kxlvREScukkFhe1fMGJx2fa1De03A5tb1HuBS1rUXwGunswYIyJicvLN7IiIaJSgiIiIRgmKiIholKCIiIhGCYqIiGiUoIiIiEYJioiIaJSgiIiIRgmKiIholKCIiIhGCYqIiGiUoIiIiEYJioiIaJSgiIiIRgmKiIholKCIiIhGkwoKST+WdEDSfkm9pXa2pN2Sni/vc2vtN0rqk/ScpFW1+mXlOH2SbiuPRKU8NvX+Ut8nqXsy442IiFM3FWcUv2N7ie2l5fMGYI/txcCe8hlJF1E9yvRiYDVwu6QzSp87gPVUz9FeXPYDrAOO274A2ALcMgXjjYiIU3A6Lj2tAXaU7R3AlbX6fbZftf0C0AcskzQPmG37UdsG7hrRZ+hYDwArhs42IiJiekw2KAx8S9KTktaX2jtsHwEo7+eV+nzgxVrf/lKbX7ZH1of1sT0IvMyIZ3QDSFovqVdS78DAwCSnFBERdbMm2f9Dtg9LOg/YLekHDW1bnQm4od7UZ3jB3gpsBVi6dOnr9kdExMRN6ozC9uHyfhT4OrAMeKlcTqK8Hy3N+4GFte4LgMOlvqBFfVgfSbOAOcCxyYw5IiJOzYSDQtJvSPqXQ9vASuAZYBewtjRbCzxUtncBPeVOpkVUi9aPl8tTJyQtL+sP143oM3Ssq4C9ZR0jIiKmyWQuPb0D+HpZW54F/KXtv5b0BLBT0jrgJ8DVALYPStoJPAsMAjfaPlmOdQNwJ3AW8HB5AWwD7pbUR3Um0TOJ8UZExARMOChs/wj4zRb1nwErRumzGdjcot4LXNKi/golaCIioj3yzeyIiGiUoIiIiEYJioiIaJSgiIiIRgmKiIholKCIiIhGCYqIiGiUoIiYBps2bWLP3vPbPYyICUlQRJxm/Ru+1+4hRExKgiIiIholKCIiolGCIiIiGiUoIiKiUYIiIiIaJSgiIqJRgiLe9C7dcSmH3ndhu4cRMWNN5lGoCyV9W9IhSQclfabUN0n6qaT95fXRWp+NkvokPSdpVa1+maQDZd9t5ZGolMem3l/q+yR1T2KuERExAZM5oxgEPmf7QmA5cKOki8q+LbaXlNc3Acq+HuBiYDVwu6QzSvs7gPVUz9FeXPYDrAOO274A2ALcMonxRkTEBEw4KGwfsf1U2T4BHALmN3RZA9xn+1XbLwB9wDJJ84DZth+1beAu4Mpanx1l+wFgxdDZRkRETI8pWaMol4Q+AOwrpU9JelrSdklzS20+8GKtW3+pzS/bI+vD+tgeBF4GzpmKMUdExPhMOigkvQ14EPis7Z9TXUY6H1gCHAFuHWraorsb6k19Ro5hvaReSb0DAwOnNoGIiGg0qaCQ9BaqkLjH9tcAbL9k+6TtXwFfBZaV5v3Awlr3BcDhUl/Qoj6sj6RZwBzg2Mhx2N5qe6ntpV1dXZOZUkREjDCZu54EbAMO2f5yrT6v1uzjwDNlexfQU+5kWkS1aP247SPACUnLyzGvAx6q9Vlbtq8C9pZ1jIiImCazJtH3Q8C1wAFJ+0vt88A1kpZQXSL6MfBJANsHJe0EnqW6Y+pG2ydLvxuAO4GzgIfLC6ogultSH9WZRM8kxhsRERMw4aCw/Te0XkP4ZkOfzcDmFvVe4JIW9VeAqyc6xoiImLx8MzsiIholKCIiolGCIiIiGiUoIiKiUYIiIiIaJSgiptmtn7iC/g3fa/cwIsYtQRExjb5y/d52DyHilCUoIiKiUYIiIiIaJSgiIqJRgiLelL5y/V5u/cQV7R5GREdIUERERKMERURENEpQREREowRFREQ0SlBEtNGh912YL+HFjJegiGiTS3dc2u4hRIxLRwSFpNWSnpPUJ2lDu8cTEfFmMuODQtIZwFeA3wcuonom90XtHVV0ov4N32PTpk3tHkZEx5nxQQEsA/ps/8j2L4H7gDVtHlPElOre8A3YNCeXo2JGku12j6GRpKuA1bb/S/l8LfBB25+qtVkPrC8f3wv8DPjH6R7rFDuXzGEmyBxmhszh9PtXtrta7Zg13SOZALWoDUs321uBra91kHptLz3dAzudMoeZIXOYGTKH9uqES0/9wMLa5wXA4TaNJSLiTacTguIJYLGkRZLeCvQAu9o8poiIN40Zf+nJ9qCkTwGPAGcA220fHKPb1jH2d4LMYWbIHGaGzKGNZvxidkREtFcnXHqKiIg2SlBERESjN1xQSPp0+bmPg5L+qFbfWH4C5DlJq9o5xvGQ9N8kWdK5tVpHzEHSH0v6gaSnJX1d0ttr+zplDh33szGSFkr6tqRD5d//z5T62ZJ2S3q+vM9t91jHIukMSd+X9H/K546ag6S3S3qg/HdwSNK/7rQ51L2hgkLS71B9a/v9ti8G/kepX0R1t9TFwGrg9vLTIDOSpIXA7wE/qdU6aQ67gUtsvx/4O2AjdM4cOvhnYwaBz9m+EFgO3FjGvQHYY3sxsKd8nuk+Axyqfe60OfwJ8Ne23wf8JtVcOm0Or3lDBQVwA/Al268C2D5a6muA+2y/avsFoI/qp0Fmqi3Af2f4Fws7Zg62v2V7sHx8jOq7L9A5c+jIn42xfcT2U2X7BNX/Oc2nGvuO0mwHcGVbBjhOkhYA/xb481q5Y+YgaTbwEWAbgO1f2v5/dNAcRnqjBcV7gA9L2ifpO5J+u9TnAy/W2vWX2owj6WPAT23/7YhdHTOHEf4AeLhsd8ocOmWco5LUDXwA2Ae8w/YRqMIEOK+NQxuP/0n1h9KvarVOmsO7gQHgL8rlsz+X9Bt01hyGmfHfoxhJ0v8F3tli1xeo5jOX6rT7t4Gdkt7NOH4GZDqNMYfPAytbdWtRm5FzsP1QafMFqssh9wx1a9F+Jt6f3SnjbEnS24AHgc/a/rnUajozk6QrgKO2n5R0eZuHM1GzgN8CPm17n6Q/oYMuM7XScUFh+3dH2yfpBuBrrr4c8rikX1H9ENeM+hmQ0eYg6VJgEfC35T/uBcBTkpbRIXMYImktcAWwwr/+ss6MmkODThnn60h6C1VI3GP7a6X8kqR5to9ImgccHf0Ibfch4GOSPgr8c2C2pP9FZ82hH+i3va98foAqKDppDsO80S49/RXwbwAkvQd4K9WvNe4CeiSdKWkRsBh4vF2DHI3tA7bPs91tu5vqX7jfsv0PdMgcoLpjCPhD4GO2f1Hb1Slz6MifjVH118U24JDtL9d27QLWlu21wEPTPbbxsr3R9oLy738PsNf2f6Sz5vAPwIuS3ltKK4Bn6aA5jNRxZxRj2A5sl/QM8Etgbflr9qCknVT/Yw0CN9o+2cZxnjLbnTSHPwXOBHaXM6PHbF/fKXOY4M/GzAQfAq4FDkjaX2qfB75EdRl2HdWddFe3Z3iT0mlz+DRwT/lD40fAf6b6w7yT5vCa/IRHREQ0eqNdeoqIiCmWoIiIiEYJioiIaJSgiIiIRgmKiIholKCIiIhGCYqIiGj0/wE9RHWuEmpX+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(input_pts[:,:,0], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "desirable-helena",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(input_pts[:,:,1], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "martial-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply standardization on numerical features\n",
    "# fit on training data column\n",
    "scale_dx = StandardScaler().fit(input_pts[:,:,0])\n",
    "scale_dy = StandardScaler().fit(input_pts[:,:,1])\n",
    "\n",
    "# transform the training data column\n",
    "input_pts[:,:,0] = scale_dx.transform(input_pts[:,:,0])\n",
    "input_pts[:,:,1] = scale_dy.transform(input_pts[:,:,1])\n",
    "\n",
    "# MinMax scale\n",
    "scaler_x = MinMaxScaler().fit(input_pts[:,:,2])\n",
    "scaler_y = MinMaxScaler().fit(input_pts[:,:,3])\n",
    "scaler_z = MinMaxScaler().fit(input_pts[:,:,4])\n",
    "\n",
    "# transform\n",
    "input_pts[:,:,2] = scaler_x.transform(input_pts[:,:,2])\n",
    "input_pts[:,:,3] = scaler_y.transform(input_pts[:,:,3])\n",
    "input_pts[:,:,4] = scaler_z.transform(input_pts[:,:,4])\n",
    "\n",
    "# # transform the testing data column\n",
    "# X_test_stand[i] = scale.transform(X_test_stand[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consecutive-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructor(model_name=\"GRU\", \n",
    "                hidden_state=10,\n",
    "                lookback=100, \n",
    "                input_vars=5,\n",
    "                output_vars=5,\n",
    "                loss=\"mae\", \n",
    "                optimizer=keras.optimizers.Adam()):\n",
    "    \n",
    "    # model instance initialization\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # add a core layer\n",
    "    if model_name == \"GRU\":\n",
    "        model.add(keras.layers.GRU(hidden_state, return_sequences=False, input_shape=(lookback, input_vars)))\n",
    "    elif model_name == \"LSTM\":\n",
    "        model.add(keras.layers.LSTM(hidden_state, return_sequences=False, input_shape=(lookback, input_vars)))\n",
    "    \n",
    "    # add the Dense layer on top\n",
    "    model.add(keras.layers.Dense(output_vars))\n",
    "    \n",
    "    # compilation\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "national-blues",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/costa/anaconda3/envs/loc_ai/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 535\n",
      "Trainable params: 535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = constructor()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lasting-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a custom callbacks list consisting of Early Stopping callback\n",
    "custom_callbacks = [EarlyStopping(monitor='loss',mode='min', patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ideal-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=3):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cardiovascular-kenya",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 825126 samples, validate on 275042 samples\n",
      "WARNING:tensorflow:From /home/costa/anaconda3/envs/loc_ai/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/1000\n",
      "825126/825126 [==============================] - 81s 98us/sample - loss: 38515.5784 - val_loss: 36490.3306\n",
      "Epoch 2/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38514.3236 - val_loss: 36488.5140\n",
      "Epoch 3/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38512.9106 - val_loss: 36487.5843\n",
      "Epoch 4/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38512.1109 - val_loss: 36486.8586\n",
      "Epoch 5/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38511.4200 - val_loss: 36486.1953\n",
      "Epoch 6/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38510.7737 - val_loss: 36485.5629\n",
      "Epoch 7/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38510.1509 - val_loss: 36484.9497\n",
      "Epoch 8/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38509.5458 - val_loss: 36484.3500\n",
      "Epoch 9/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38508.9506 - val_loss: 36483.7590\n",
      "Epoch 10/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38508.3628 - val_loss: 36483.1740\n",
      "Epoch 11/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38507.7817 - val_loss: 36482.5964\n",
      "Epoch 12/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38507.2049 - val_loss: 36482.0217\n",
      "Epoch 13/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38506.6323 - val_loss: 36481.4509\n",
      "Epoch 14/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38506.0640 - val_loss: 36480.8848\n",
      "Epoch 15/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38505.4981 - val_loss: 36480.3199\n",
      "Epoch 16/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38504.9330 - val_loss: 36479.7560\n",
      "Epoch 17/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38504.3705 - val_loss: 36479.1955\n",
      "Epoch 18/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38503.8104 - val_loss: 36478.6341\n",
      "Epoch 19/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38503.2508 - val_loss: 36478.0741\n",
      "Epoch 20/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38502.6913 - val_loss: 36477.5160\n",
      "Epoch 21/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38502.1341 - val_loss: 36476.9595\n",
      "Epoch 22/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38501.5786 - val_loss: 36476.4060\n",
      "Epoch 23/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38501.0229 - val_loss: 36475.8485\n",
      "Epoch 24/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38500.4682 - val_loss: 36475.2966\n",
      "Epoch 25/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38499.9135 - val_loss: 36474.7413\n",
      "Epoch 26/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38499.3595 - val_loss: 36474.1879\n",
      "Epoch 27/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38498.8073 - val_loss: 36473.6341\n",
      "Epoch 28/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38498.2550 - val_loss: 36473.0824\n",
      "Epoch 29/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38497.7025 - val_loss: 36472.5304\n",
      "Epoch 30/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38497.1505 - val_loss: 36471.9781\n",
      "Epoch 31/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38496.5993 - val_loss: 36471.4277\n",
      "Epoch 32/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38496.0479 - val_loss: 36470.8776\n",
      "Epoch 33/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38495.4972 - val_loss: 36470.3272\n",
      "Epoch 34/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38494.9469 - val_loss: 36469.7755\n",
      "Epoch 35/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38494.3972 - val_loss: 36469.2260\n",
      "Epoch 36/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38493.8464 - val_loss: 36468.6756\n",
      "Epoch 37/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38493.2966 - val_loss: 36468.1256\n",
      "Epoch 38/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38492.7467 - val_loss: 36467.5762\n",
      "Epoch 39/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38492.1959 - val_loss: 36467.0270\n",
      "Epoch 40/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38491.6467 - val_loss: 36466.4772\n",
      "Epoch 41/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38491.0980 - val_loss: 36465.9273\n",
      "Epoch 42/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38490.5486 - val_loss: 36465.3787\n",
      "Epoch 43/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38489.9994 - val_loss: 36464.8324\n",
      "Epoch 44/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38489.4511 - val_loss: 36464.2800\n",
      "Epoch 45/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38488.9020 - val_loss: 36463.7323\n",
      "Epoch 46/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38488.3542 - val_loss: 36463.1834\n",
      "Epoch 47/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38487.8053 - val_loss: 36462.6342\n",
      "Epoch 48/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38487.2559 - val_loss: 36462.0865\n",
      "Epoch 49/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38486.7072 - val_loss: 36461.5387\n",
      "Epoch 50/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38486.1590 - val_loss: 36460.9876\n",
      "Epoch 51/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38485.6106 - val_loss: 36460.4392\n",
      "Epoch 52/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38485.0628 - val_loss: 36459.8910\n",
      "Epoch 53/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38484.5140 - val_loss: 36459.3442\n",
      "Epoch 54/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38483.9655 - val_loss: 36458.7945\n",
      "Epoch 55/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38483.4177 - val_loss: 36458.2463\n",
      "Epoch 56/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38482.8697 - val_loss: 36457.6965\n",
      "Epoch 57/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38482.3217 - val_loss: 36457.1508\n",
      "Epoch 58/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38481.7732 - val_loss: 36456.6058\n",
      "Epoch 59/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38481.2251 - val_loss: 36456.0558\n",
      "Epoch 60/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38480.6776 - val_loss: 36455.5075\n",
      "Epoch 61/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38480.1290 - val_loss: 36454.9590\n",
      "Epoch 62/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38479.5822 - val_loss: 36454.4120\n",
      "Epoch 63/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38479.0340 - val_loss: 36453.8627\n",
      "Epoch 64/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38478.4862 - val_loss: 36453.3175\n",
      "Epoch 65/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38477.9375 - val_loss: 36452.7689\n",
      "Epoch 66/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38477.3905 - val_loss: 36452.2212\n",
      "Epoch 67/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38476.8430 - val_loss: 36451.6732\n",
      "Epoch 68/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38476.2950 - val_loss: 36451.1264\n",
      "Epoch 69/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38475.7484 - val_loss: 36450.5793\n",
      "Epoch 70/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38475.2001 - val_loss: 36450.0317\n",
      "Epoch 71/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38474.6531 - val_loss: 36449.4859\n",
      "Epoch 72/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38474.1056 - val_loss: 36448.9390\n",
      "Epoch 73/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38473.5586 - val_loss: 36448.3896\n",
      "Epoch 74/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38473.0116 - val_loss: 36447.8418\n",
      "Epoch 75/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38472.4645 - val_loss: 36447.2967\n",
      "Epoch 76/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38471.9186 - val_loss: 36446.7502\n",
      "Epoch 77/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38471.3709 - val_loss: 36446.2029\n",
      "Epoch 78/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38470.8246 - val_loss: 36445.6558\n",
      "Epoch 79/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38470.2775 - val_loss: 36445.1097\n",
      "Epoch 80/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38469.7315 - val_loss: 36444.5639\n",
      "Epoch 81/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38469.1848 - val_loss: 36444.0173\n",
      "Epoch 82/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38468.6378 - val_loss: 36443.4696\n",
      "Epoch 83/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38468.0912 - val_loss: 36442.9257\n",
      "Epoch 84/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38467.5455 - val_loss: 36442.3790\n",
      "Epoch 85/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38466.9997 - val_loss: 36441.8330\n",
      "Epoch 86/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38466.4535 - val_loss: 36441.2875\n",
      "Epoch 87/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38465.9077 - val_loss: 36440.7405\n",
      "Epoch 88/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38465.3613 - val_loss: 36440.1951\n",
      "Epoch 89/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38464.8166 - val_loss: 36439.6498\n",
      "Epoch 90/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38464.2703 - val_loss: 36439.1050\n",
      "Epoch 91/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38463.7253 - val_loss: 36438.5602\n",
      "Epoch 92/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38463.1803 - val_loss: 36438.0147\n",
      "Epoch 93/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38462.6356 - val_loss: 36437.4680\n",
      "Epoch 94/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38462.0898 - val_loss: 36436.9258\n",
      "Epoch 95/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38461.5455 - val_loss: 36436.3812\n",
      "Epoch 96/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38461.0018 - val_loss: 36435.8357\n",
      "Epoch 97/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38460.4567 - val_loss: 36435.2920\n",
      "Epoch 98/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38459.9126 - val_loss: 36434.7483\n",
      "Epoch 99/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38459.3688 - val_loss: 36434.2027\n",
      "Epoch 100/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38458.8241 - val_loss: 36433.6564\n",
      "Epoch 101/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38458.2808 - val_loss: 36433.1160\n",
      "Epoch 102/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38457.7374 - val_loss: 36432.5715\n",
      "Epoch 103/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38457.1951 - val_loss: 36432.0285\n",
      "Epoch 104/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38456.6514 - val_loss: 36431.4866\n",
      "Epoch 105/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38456.1082 - val_loss: 36430.9419\n",
      "Epoch 106/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38455.5658 - val_loss: 36430.4000\n",
      "Epoch 107/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38455.0242 - val_loss: 36429.8579\n",
      "Epoch 108/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38454.4818 - val_loss: 36429.3157\n",
      "Epoch 109/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38453.9404 - val_loss: 36428.7724\n",
      "Epoch 110/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38453.3978 - val_loss: 36428.2291\n",
      "Epoch 111/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38452.8557 - val_loss: 36427.6887\n",
      "Epoch 112/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38452.3150 - val_loss: 36427.1491\n",
      "Epoch 113/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38451.7745 - val_loss: 36426.6072\n",
      "Epoch 114/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38451.2336 - val_loss: 36426.0652\n",
      "Epoch 115/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38450.6930 - val_loss: 36425.5245\n",
      "Epoch 116/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38450.1531 - val_loss: 36424.9858\n",
      "Epoch 117/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38449.6137 - val_loss: 36424.4449\n",
      "Epoch 118/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38449.0735 - val_loss: 36423.9048\n",
      "Epoch 119/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38448.5336 - val_loss: 36423.3641\n",
      "Epoch 120/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38447.9950 - val_loss: 36422.8262\n",
      "Epoch 121/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38447.4569 - val_loss: 36422.2871\n",
      "Epoch 122/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38446.9185 - val_loss: 36421.7496\n",
      "Epoch 123/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38446.3804 - val_loss: 36421.2109\n",
      "Epoch 124/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38445.8421 - val_loss: 36420.6726\n",
      "Epoch 125/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38445.3051 - val_loss: 36420.1351\n",
      "Epoch 126/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38444.7690 - val_loss: 36419.5986\n",
      "Epoch 127/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38444.2317 - val_loss: 36419.0583\n",
      "Epoch 128/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38443.6957 - val_loss: 36418.5230\n",
      "Epoch 129/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38443.1594 - val_loss: 36417.9883\n",
      "Epoch 130/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38442.6239 - val_loss: 36417.4510\n",
      "Epoch 131/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38442.0894 - val_loss: 36416.9189\n",
      "Epoch 132/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38441.5544 - val_loss: 36416.3816\n",
      "Epoch 133/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38441.0212 - val_loss: 36415.8472\n",
      "Epoch 134/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38440.4874 - val_loss: 36415.3155\n",
      "Epoch 135/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38439.9534 - val_loss: 36414.7819\n",
      "Epoch 136/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38439.4206 - val_loss: 36414.2478\n",
      "Epoch 137/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38438.8892 - val_loss: 36413.7144\n",
      "Epoch 138/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38438.3569 - val_loss: 36413.1830\n",
      "Epoch 139/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38437.8266 - val_loss: 36412.6529\n",
      "Epoch 140/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38437.2945 - val_loss: 36412.1226\n",
      "Epoch 141/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38436.7656 - val_loss: 36411.5918\n",
      "Epoch 142/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38436.2350 - val_loss: 36411.0610\n",
      "Epoch 143/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38435.7054 - val_loss: 36410.5335\n",
      "Epoch 144/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38435.1777 - val_loss: 36410.0025\n",
      "Epoch 145/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38434.6499 - val_loss: 36409.4748\n",
      "Epoch 146/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38434.1229 - val_loss: 36408.9472\n",
      "Epoch 147/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38433.5969 - val_loss: 36408.4208\n",
      "Epoch 148/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38433.0710 - val_loss: 36407.8944\n",
      "Epoch 149/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38432.5447 - val_loss: 36407.3699\n",
      "Epoch 150/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38432.0206 - val_loss: 36406.8437\n",
      "Epoch 151/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38431.4963 - val_loss: 36406.3193\n",
      "Epoch 152/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38430.9742 - val_loss: 36405.7985\n",
      "Epoch 153/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38430.4512 - val_loss: 36405.2762\n",
      "Epoch 154/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38429.9298 - val_loss: 36404.7512\n",
      "Epoch 155/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38429.4087 - val_loss: 36404.2308\n",
      "Epoch 156/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38428.8886 - val_loss: 36403.7107\n",
      "Epoch 157/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38428.3681 - val_loss: 36403.1919\n",
      "Epoch 158/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38427.8506 - val_loss: 36402.6742\n",
      "Epoch 159/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38427.3327 - val_loss: 36402.1514\n",
      "Epoch 160/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38426.8158 - val_loss: 36401.6379\n",
      "Epoch 161/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38426.2991 - val_loss: 36401.1216\n",
      "Epoch 162/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38425.7850 - val_loss: 36400.6062\n",
      "Epoch 163/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38425.2694 - val_loss: 36400.0893\n",
      "Epoch 164/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38424.7556 - val_loss: 36399.5761\n",
      "Epoch 165/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38424.2417 - val_loss: 36399.0632\n",
      "Epoch 166/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38423.7305 - val_loss: 36398.5514\n",
      "Epoch 167/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38423.2185 - val_loss: 36398.0397\n",
      "Epoch 168/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38422.7084 - val_loss: 36397.5293\n",
      "Epoch 169/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38422.1992 - val_loss: 36397.0200\n",
      "Epoch 170/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38421.6900 - val_loss: 36396.5098\n",
      "Epoch 171/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38421.1822 - val_loss: 36396.0014\n",
      "Epoch 172/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38420.6758 - val_loss: 36395.4933\n",
      "Epoch 173/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38420.1696 - val_loss: 36394.9873\n",
      "Epoch 174/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38419.6645 - val_loss: 36394.4839\n",
      "Epoch 175/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38419.1595 - val_loss: 36393.9794\n",
      "Epoch 176/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38418.6570 - val_loss: 36393.4748\n",
      "Epoch 177/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38418.1535 - val_loss: 36392.9708\n",
      "Epoch 178/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38417.6523 - val_loss: 36392.4696\n",
      "Epoch 179/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38417.1521 - val_loss: 36391.9702\n",
      "Epoch 180/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38416.6526 - val_loss: 36391.4707\n",
      "Epoch 181/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38416.1534 - val_loss: 36390.9705\n",
      "Epoch 182/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38415.6562 - val_loss: 36390.4725\n",
      "Epoch 183/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38415.1585 - val_loss: 36389.9773\n",
      "Epoch 184/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38414.6627 - val_loss: 36389.4822\n",
      "Epoch 185/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38414.1673 - val_loss: 36388.9858\n",
      "Epoch 186/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38413.6731 - val_loss: 36388.4924\n",
      "Epoch 187/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38413.1801 - val_loss: 36387.9987\n",
      "Epoch 188/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38412.6885 - val_loss: 36387.5074\n",
      "Epoch 189/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38412.1961 - val_loss: 36387.0185\n",
      "Epoch 190/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38411.7063 - val_loss: 36386.5267\n",
      "Epoch 191/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38411.2170 - val_loss: 36386.0377\n",
      "Epoch 192/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38410.7288 - val_loss: 36385.5511\n",
      "Epoch 193/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38410.2402 - val_loss: 36385.0640\n",
      "Epoch 194/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38409.7543 - val_loss: 36384.5787\n",
      "Epoch 195/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38409.2683 - val_loss: 36384.0933\n",
      "Epoch 196/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38408.7821 - val_loss: 36383.6081\n",
      "Epoch 197/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38408.2985 - val_loss: 36383.1259\n",
      "Epoch 198/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38407.8159 - val_loss: 36382.6456\n",
      "Epoch 199/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38407.3337 - val_loss: 36382.1641\n",
      "Epoch 200/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38406.8515 - val_loss: 36381.6821\n",
      "Epoch 201/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38406.3711 - val_loss: 36381.2043\n",
      "Epoch 202/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38405.8923 - val_loss: 36380.7251\n",
      "Epoch 203/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38405.4130 - val_loss: 36380.2484\n",
      "Epoch 204/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38404.9352 - val_loss: 36379.7754\n",
      "Epoch 205/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38404.4582 - val_loss: 36379.2981\n",
      "Epoch 206/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38403.9827 - val_loss: 36378.8242\n",
      "Epoch 207/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38403.5066 - val_loss: 36378.3512\n",
      "Epoch 208/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38403.0333 - val_loss: 36377.8816\n",
      "Epoch 209/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38402.5605 - val_loss: 36377.4096\n",
      "Epoch 210/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38402.0878 - val_loss: 36376.9374\n",
      "Epoch 211/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38401.6162 - val_loss: 36376.4708\n",
      "Epoch 212/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38401.1463 - val_loss: 36376.0017\n",
      "Epoch 213/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38400.6755 - val_loss: 36375.5360\n",
      "Epoch 214/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38400.2078 - val_loss: 36375.0703\n",
      "Epoch 215/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38399.7391 - val_loss: 36374.6044\n",
      "Epoch 216/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38399.2723 - val_loss: 36374.1405\n",
      "Epoch 217/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38398.8074 - val_loss: 36373.6776\n",
      "Epoch 218/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38398.3411 - val_loss: 36373.2166\n",
      "Epoch 219/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38397.8784 - val_loss: 36372.7547\n",
      "Epoch 220/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38397.4150 - val_loss: 36372.2952\n",
      "Epoch 221/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38396.9529 - val_loss: 36371.8369\n",
      "Epoch 222/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38396.4911 - val_loss: 36371.3778\n",
      "Epoch 223/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38396.0310 - val_loss: 36370.9221\n",
      "Epoch 224/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38395.5716 - val_loss: 36370.4683\n",
      "Epoch 225/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38395.1130 - val_loss: 36370.0115\n",
      "Epoch 226/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38394.6560 - val_loss: 36369.5583\n",
      "Epoch 227/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38394.1988 - val_loss: 36369.1050\n",
      "Epoch 228/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38393.7437 - val_loss: 36368.6551\n",
      "Epoch 229/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38393.2887 - val_loss: 36368.2037\n",
      "Epoch 230/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38392.8340 - val_loss: 36367.7532\n",
      "Epoch 231/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38392.3818 - val_loss: 36367.3072\n",
      "Epoch 232/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38391.9291 - val_loss: 36366.8579\n",
      "Epoch 233/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38391.4787 - val_loss: 36366.4116\n",
      "Epoch 234/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38391.0276 - val_loss: 36365.9659\n",
      "Epoch 235/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38390.5785 - val_loss: 36365.5210\n",
      "Epoch 236/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38390.1303 - val_loss: 36365.0776\n",
      "Epoch 237/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38389.6827 - val_loss: 36364.6349\n",
      "Epoch 238/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38389.2350 - val_loss: 36364.1942\n",
      "Epoch 239/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38388.7895 - val_loss: 36363.7547\n",
      "Epoch 240/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38388.3444 - val_loss: 36363.3150\n",
      "Epoch 241/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38387.9001 - val_loss: 36362.8776\n",
      "Epoch 242/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38387.4581 - val_loss: 36362.4385\n",
      "Epoch 243/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38387.0159 - val_loss: 36362.0037\n",
      "Epoch 244/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38386.5739 - val_loss: 36361.5654\n",
      "Epoch 245/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38386.1338 - val_loss: 36361.1308\n",
      "Epoch 246/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38385.6941 - val_loss: 36360.6996\n",
      "Epoch 247/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38385.2549 - val_loss: 36360.2681\n",
      "Epoch 248/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38384.8183 - val_loss: 36359.8367\n",
      "Epoch 249/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38384.3812 - val_loss: 36359.4054\n",
      "Epoch 250/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38383.9445 - val_loss: 36358.9760\n",
      "Epoch 251/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38383.5099 - val_loss: 36358.5501\n",
      "Epoch 252/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38383.0761 - val_loss: 36358.1200\n",
      "Epoch 253/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38382.6422 - val_loss: 36357.6962\n",
      "Epoch 254/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38382.2104 - val_loss: 36357.2709\n",
      "Epoch 255/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38381.7788 - val_loss: 36356.8469\n",
      "Epoch 256/1000\n",
      "825126/825126 [==============================] - 69s 84us/sample - loss: 38381.3485 - val_loss: 36356.4229\n",
      "Epoch 257/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38380.9196 - val_loss: 36356.0000\n",
      "Epoch 258/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38380.4901 - val_loss: 36355.5814\n",
      "Epoch 259/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38380.0621 - val_loss: 36355.1594\n",
      "Epoch 260/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38379.6361 - val_loss: 36354.7396\n",
      "Epoch 261/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38379.2088 - val_loss: 36354.3232\n",
      "Epoch 262/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38378.7847 - val_loss: 36353.9068\n",
      "Epoch 263/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38378.3602 - val_loss: 36353.4901\n",
      "Epoch 264/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38377.9363 - val_loss: 36353.0760\n",
      "Epoch 265/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38377.5134 - val_loss: 36352.6611\n",
      "Epoch 266/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38377.0925 - val_loss: 36352.2491\n",
      "Epoch 267/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38376.6712 - val_loss: 36351.8356\n",
      "Epoch 268/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38376.2521 - val_loss: 36351.4272\n",
      "Epoch 269/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38375.8318 - val_loss: 36351.0154\n",
      "Epoch 270/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38375.4139 - val_loss: 36350.6048\n",
      "Epoch 271/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38374.9959 - val_loss: 36350.1987\n",
      "Epoch 272/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38374.5804 - val_loss: 36349.7912\n",
      "Epoch 273/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38374.1644 - val_loss: 36349.3854\n",
      "Epoch 274/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38373.7493 - val_loss: 36348.9819\n",
      "Epoch 275/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38373.3363 - val_loss: 36348.5772\n",
      "Epoch 276/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38372.9217 - val_loss: 36348.1735\n",
      "Epoch 277/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38372.5102 - val_loss: 36347.7707\n",
      "Epoch 278/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38372.0984 - val_loss: 36347.3703\n",
      "Epoch 279/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38371.6882 - val_loss: 36346.9691\n",
      "Epoch 280/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38371.2775 - val_loss: 36346.5687\n",
      "Epoch 281/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38370.8696 - val_loss: 36346.1726\n",
      "Epoch 282/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38370.4612 - val_loss: 36345.7735\n",
      "Epoch 283/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38370.0531 - val_loss: 36345.3776\n",
      "Epoch 284/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38369.6475 - val_loss: 36344.9832\n",
      "Epoch 285/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38369.2416 - val_loss: 36344.5857\n",
      "Epoch 286/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38368.8372 - val_loss: 36344.1946\n",
      "Epoch 287/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38368.4338 - val_loss: 36343.8005\n",
      "Epoch 288/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38368.0300 - val_loss: 36343.4088\n",
      "Epoch 289/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38367.6271 - val_loss: 36343.0182\n",
      "Epoch 290/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38367.2257 - val_loss: 36342.6273\n",
      "Epoch 291/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38366.8243 - val_loss: 36342.2398\n",
      "Epoch 292/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38366.4250 - val_loss: 36341.8505\n",
      "Epoch 293/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38366.0264 - val_loss: 36341.4635\n",
      "Epoch 294/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38365.6273 - val_loss: 36341.0761\n",
      "Epoch 295/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38365.2295 - val_loss: 36340.6923\n",
      "Epoch 296/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38364.8325 - val_loss: 36340.3078\n",
      "Epoch 297/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38364.4362 - val_loss: 36339.9245\n",
      "Epoch 298/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38364.0415 - val_loss: 36339.5415\n",
      "Epoch 299/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38363.6473 - val_loss: 36339.1580\n",
      "Epoch 300/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38363.2535 - val_loss: 36338.7773\n",
      "Epoch 301/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38362.8607 - val_loss: 36338.3961\n",
      "Epoch 302/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38362.4688 - val_loss: 36338.0178\n",
      "Epoch 303/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38362.0782 - val_loss: 36337.6404\n",
      "Epoch 304/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38361.6878 - val_loss: 36337.2626\n",
      "Epoch 305/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38361.2970 - val_loss: 36336.8850\n",
      "Epoch 306/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38360.9089 - val_loss: 36336.5095\n",
      "Epoch 307/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38360.5209 - val_loss: 36336.1354\n",
      "Epoch 308/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38360.1339 - val_loss: 36335.7599\n",
      "Epoch 309/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38359.7467 - val_loss: 36335.3892\n",
      "Epoch 310/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38359.3609 - val_loss: 36335.0158\n",
      "Epoch 311/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38358.9766 - val_loss: 36334.6449\n",
      "Epoch 312/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38358.5918 - val_loss: 36334.2746\n",
      "Epoch 313/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38358.2101 - val_loss: 36333.9049\n",
      "Epoch 314/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38357.8260 - val_loss: 36333.5358\n",
      "Epoch 315/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38357.4456 - val_loss: 36333.1699\n",
      "Epoch 316/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38357.0649 - val_loss: 36332.8029\n",
      "Epoch 317/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38356.6843 - val_loss: 36332.4367\n",
      "Epoch 318/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38356.3052 - val_loss: 36332.0726\n",
      "Epoch 319/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38355.9265 - val_loss: 36331.7070\n",
      "Epoch 320/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38355.5479 - val_loss: 36331.3435\n",
      "Epoch 321/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38355.1712 - val_loss: 36330.9816\n",
      "Epoch 322/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38354.7945 - val_loss: 36330.6186\n",
      "Epoch 323/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38354.4197 - val_loss: 36330.2578\n",
      "Epoch 324/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38354.0449 - val_loss: 36329.8984\n",
      "Epoch 325/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38353.6709 - val_loss: 36329.5392\n",
      "Epoch 326/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38353.2977 - val_loss: 36329.1811\n",
      "Epoch 327/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38352.9252 - val_loss: 36328.8224\n",
      "Epoch 328/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38352.5534 - val_loss: 36328.4666\n",
      "Epoch 329/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38352.1830 - val_loss: 36328.1113\n",
      "Epoch 330/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38351.8126 - val_loss: 36327.7545\n",
      "Epoch 331/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38351.4434 - val_loss: 36327.4014\n",
      "Epoch 332/1000\n",
      "825126/825126 [==============================] - 69s 84us/sample - loss: 38351.0746 - val_loss: 36327.0465\n",
      "Epoch 333/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38350.7074 - val_loss: 36326.6952\n",
      "Epoch 334/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38350.3401 - val_loss: 36326.3432\n",
      "Epoch 335/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38349.9744 - val_loss: 36325.9901\n",
      "Epoch 336/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38349.6087 - val_loss: 36325.6411\n",
      "Epoch 337/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38349.2449 - val_loss: 36325.2895\n",
      "Epoch 338/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38348.8799 - val_loss: 36324.9434\n",
      "Epoch 339/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38348.5182 - val_loss: 36324.5952\n",
      "Epoch 340/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38348.1570 - val_loss: 36324.2479\n",
      "Epoch 341/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38347.7948 - val_loss: 36323.9020\n",
      "Epoch 342/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38347.4338 - val_loss: 36323.5547\n",
      "Epoch 343/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38347.0735 - val_loss: 36323.2111\n",
      "Epoch 344/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38346.7151 - val_loss: 36322.8668\n",
      "Epoch 345/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38346.3570 - val_loss: 36322.5243\n",
      "Epoch 346/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38345.9987 - val_loss: 36322.1813\n",
      "Epoch 347/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38345.6423 - val_loss: 36321.8407\n",
      "Epoch 348/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38345.2864 - val_loss: 36321.4998\n",
      "Epoch 349/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38344.9312 - val_loss: 36321.1603\n",
      "Epoch 350/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38344.5763 - val_loss: 36320.8171\n",
      "Epoch 351/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38344.2218 - val_loss: 36320.4801\n",
      "Epoch 352/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38343.8683 - val_loss: 36320.1409\n",
      "Epoch 353/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38343.5160 - val_loss: 36319.8043\n",
      "Epoch 354/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38343.1650 - val_loss: 36319.4696\n",
      "Epoch 355/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38342.8133 - val_loss: 36319.1308\n",
      "Epoch 356/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38342.4625 - val_loss: 36318.7967\n",
      "Epoch 357/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38342.1140 - val_loss: 36318.4632\n",
      "Epoch 358/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38341.7649 - val_loss: 36318.1301\n",
      "Epoch 359/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38341.4159 - val_loss: 36317.7960\n",
      "Epoch 360/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38341.0689 - val_loss: 36317.4620\n",
      "Epoch 361/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38340.7220 - val_loss: 36317.1310\n",
      "Epoch 362/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38340.3758 - val_loss: 36316.7999\n",
      "Epoch 363/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38340.0297 - val_loss: 36316.4709\n",
      "Epoch 364/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38339.6857 - val_loss: 36316.1416\n",
      "Epoch 365/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38339.3416 - val_loss: 36315.8112\n",
      "Epoch 366/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38338.9982 - val_loss: 36315.4829\n",
      "Epoch 367/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38338.6559 - val_loss: 36315.1570\n",
      "Epoch 368/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38338.3135 - val_loss: 36314.8317\n",
      "Epoch 369/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38337.9725 - val_loss: 36314.5039\n",
      "Epoch 370/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38337.6318 - val_loss: 36314.1773\n",
      "Epoch 371/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38337.2928 - val_loss: 36313.8558\n",
      "Epoch 372/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38336.9532 - val_loss: 36313.5313\n",
      "Epoch 373/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38336.6140 - val_loss: 36313.2079\n",
      "Epoch 374/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38336.2770 - val_loss: 36312.8865\n",
      "Epoch 375/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38335.9392 - val_loss: 36312.5641\n",
      "Epoch 376/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38335.6032 - val_loss: 36312.2426\n",
      "Epoch 377/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38335.2677 - val_loss: 36311.9226\n",
      "Epoch 378/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38334.9338 - val_loss: 36311.6006\n",
      "Epoch 379/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38334.5987 - val_loss: 36311.2820\n",
      "Epoch 380/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38334.2658 - val_loss: 36310.9619\n",
      "Epoch 381/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38333.9328 - val_loss: 36310.6453\n",
      "Epoch 382/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38333.6004 - val_loss: 36310.3280\n",
      "Epoch 383/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38333.2693 - val_loss: 36310.0141\n",
      "Epoch 384/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38332.9394 - val_loss: 36309.6990\n",
      "Epoch 385/1000\n",
      "825126/825126 [==============================] - 69s 84us/sample - loss: 38332.6083 - val_loss: 36309.3816\n",
      "Epoch 386/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38332.2792 - val_loss: 36309.0672\n",
      "Epoch 387/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38331.9507 - val_loss: 36308.7547\n",
      "Epoch 388/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38331.6228 - val_loss: 36308.4429\n",
      "Epoch 389/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38331.2954 - val_loss: 36308.1294\n",
      "Epoch 390/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38330.9688 - val_loss: 36307.8176\n",
      "Epoch 391/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38330.6428 - val_loss: 36307.5075\n",
      "Epoch 392/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38330.3176 - val_loss: 36307.1992\n",
      "Epoch 393/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38329.9939 - val_loss: 36306.8881\n",
      "Epoch 394/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38329.6705 - val_loss: 36306.5814\n",
      "Epoch 395/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38329.3472 - val_loss: 36306.2733\n",
      "Epoch 396/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38329.0259 - val_loss: 36305.9662\n",
      "Epoch 397/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38328.7042 - val_loss: 36305.6591\n",
      "Epoch 398/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38328.3826 - val_loss: 36305.3535\n",
      "Epoch 399/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38328.0622 - val_loss: 36305.0500\n",
      "Epoch 400/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38327.7432 - val_loss: 36304.7415\n",
      "Epoch 401/1000\n",
      "825126/825126 [==============================] - 69s 84us/sample - loss: 38327.4249 - val_loss: 36304.4383\n",
      "Epoch 402/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38327.1069 - val_loss: 36304.1364\n",
      "Epoch 403/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38326.7890 - val_loss: 36303.8336\n",
      "Epoch 404/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38326.4725 - val_loss: 36303.5349\n",
      "Epoch 405/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38326.1566 - val_loss: 36303.2322\n",
      "Epoch 406/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38325.8409 - val_loss: 36302.9310\n",
      "Epoch 407/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38325.5264 - val_loss: 36302.6305\n",
      "Epoch 408/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38325.2135 - val_loss: 36302.3339\n",
      "Epoch 409/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38324.9008 - val_loss: 36302.0347\n",
      "Epoch 410/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38324.5875 - val_loss: 36301.7366\n",
      "Epoch 411/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38324.2765 - val_loss: 36301.4397\n",
      "Epoch 412/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38323.9658 - val_loss: 36301.1444\n",
      "Epoch 413/1000\n",
      "825126/825126 [==============================] - 69s 84us/sample - loss: 38323.6557 - val_loss: 36300.8493\n",
      "Epoch 414/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38323.3467 - val_loss: 36300.5544\n",
      "Epoch 415/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38323.0381 - val_loss: 36300.2608\n",
      "Epoch 416/1000\n",
      "825126/825126 [==============================] - 69s 84us/sample - loss: 38322.7304 - val_loss: 36299.9673\n",
      "Epoch 417/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38322.4227 - val_loss: 36299.6727\n",
      "Epoch 418/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38322.1161 - val_loss: 36299.3822\n",
      "Epoch 419/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38321.8114 - val_loss: 36299.0906\n",
      "Epoch 420/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38321.5063 - val_loss: 36298.7989\n",
      "Epoch 421/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38321.2017 - val_loss: 36298.5102\n",
      "Epoch 422/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38320.8991 - val_loss: 36298.2206\n",
      "Epoch 423/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38320.5965 - val_loss: 36297.9348\n",
      "Epoch 424/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38320.2946 - val_loss: 36297.6467\n",
      "Epoch 425/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38319.9938 - val_loss: 36297.3602\n",
      "Epoch 426/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38319.6935 - val_loss: 36297.0740\n",
      "Epoch 427/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38319.3942 - val_loss: 36296.7861\n",
      "Epoch 428/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38319.0955 - val_loss: 36296.5053\n",
      "Epoch 429/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38318.7969 - val_loss: 36296.2207\n",
      "Epoch 430/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38318.5001 - val_loss: 36295.9368\n",
      "Epoch 431/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38318.2031 - val_loss: 36295.6551\n",
      "Epoch 432/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38317.9077 - val_loss: 36295.3733\n",
      "Epoch 433/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38317.6125 - val_loss: 36295.0942\n",
      "Epoch 434/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38317.3190 - val_loss: 36294.8138\n",
      "Epoch 435/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38317.0252 - val_loss: 36294.5321\n",
      "Epoch 436/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38316.7332 - val_loss: 36294.2544\n",
      "Epoch 437/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38316.4407 - val_loss: 36293.9749\n",
      "Epoch 438/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38316.1494 - val_loss: 36293.6983\n",
      "Epoch 439/1000\n",
      "825126/825126 [==============================] - 69s 84us/sample - loss: 38315.8577 - val_loss: 36293.4221\n",
      "Epoch 440/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38315.5688 - val_loss: 36293.1444\n",
      "Epoch 441/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38315.2798 - val_loss: 36292.8702\n",
      "Epoch 442/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38314.9919 - val_loss: 36292.5945\n",
      "Epoch 443/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38314.7044 - val_loss: 36292.3218\n",
      "Epoch 444/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38314.4183 - val_loss: 36292.0481\n",
      "Epoch 445/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38314.1314 - val_loss: 36291.7731\n",
      "Epoch 446/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38313.8461 - val_loss: 36291.5042\n",
      "Epoch 447/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38313.5604 - val_loss: 36291.2319\n",
      "Epoch 448/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38313.2767 - val_loss: 36290.9639\n",
      "Epoch 449/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38312.9948 - val_loss: 36290.6926\n",
      "Epoch 450/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38312.7119 - val_loss: 36290.4219\n",
      "Epoch 451/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38312.4295 - val_loss: 36290.1541\n",
      "Epoch 452/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38312.1490 - val_loss: 36289.8862\n",
      "Epoch 453/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38311.8683 - val_loss: 36289.6196\n",
      "Epoch 454/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38311.5887 - val_loss: 36289.3523\n",
      "Epoch 455/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38311.3105 - val_loss: 36289.0878\n",
      "Epoch 456/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38311.0314 - val_loss: 36288.8225\n",
      "Epoch 457/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38310.7536 - val_loss: 36288.5544\n",
      "Epoch 458/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38310.4762 - val_loss: 36288.2925\n",
      "Epoch 459/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38310.2000 - val_loss: 36288.0292\n",
      "Epoch 460/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38309.9247 - val_loss: 36287.7656\n",
      "Epoch 461/1000\n",
      "825126/825126 [==============================] - 69s 84us/sample - loss: 38309.6498 - val_loss: 36287.5026\n",
      "Epoch 462/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38309.3741 - val_loss: 36287.2400\n",
      "Epoch 463/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38309.1004 - val_loss: 36286.9796\n",
      "Epoch 464/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38308.8267 - val_loss: 36286.7186\n",
      "Epoch 465/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38308.5549 - val_loss: 36286.4588\n",
      "Epoch 466/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38308.2810 - val_loss: 36286.1969\n",
      "Epoch 467/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38308.0100 - val_loss: 36285.9396\n",
      "Epoch 468/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38307.7397 - val_loss: 36285.6808\n",
      "Epoch 469/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38307.4688 - val_loss: 36285.4228\n",
      "Epoch 470/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38307.1989 - val_loss: 36285.1646\n",
      "Epoch 471/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38306.9299 - val_loss: 36284.9081\n",
      "Epoch 472/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38306.6611 - val_loss: 36284.6498\n",
      "Epoch 473/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38306.3943 - val_loss: 36284.3963\n",
      "Epoch 474/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38306.1262 - val_loss: 36284.1401\n",
      "Epoch 475/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38305.8581 - val_loss: 36283.8830\n",
      "Epoch 476/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38305.5922 - val_loss: 36283.6282\n",
      "Epoch 477/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38305.3261 - val_loss: 36283.3745\n",
      "Epoch 478/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38305.0609 - val_loss: 36283.1212\n",
      "Epoch 479/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38304.7955 - val_loss: 36282.8671\n",
      "Epoch 480/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38304.5310 - val_loss: 36282.6163\n",
      "Epoch 481/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38304.2680 - val_loss: 36282.3625\n",
      "Epoch 482/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38304.0043 - val_loss: 36282.1095\n",
      "Epoch 483/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38303.7412 - val_loss: 36281.8587\n",
      "Epoch 484/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38303.4782 - val_loss: 36281.6056\n",
      "Epoch 485/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38303.2168 - val_loss: 36281.3556\n",
      "Epoch 486/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38302.9553 - val_loss: 36281.1039\n",
      "Epoch 487/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38302.6941 - val_loss: 36280.8550\n",
      "Epoch 488/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38302.4337 - val_loss: 36280.6067\n",
      "Epoch 489/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38302.1734 - val_loss: 36280.3552\n",
      "Epoch 490/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38301.9130 - val_loss: 36280.1046\n",
      "Epoch 491/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38301.6542 - val_loss: 36279.8566\n",
      "Epoch 492/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38301.3954 - val_loss: 36279.6102\n",
      "Epoch 493/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38301.1377 - val_loss: 36279.3635\n",
      "Epoch 494/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38300.8797 - val_loss: 36279.1137\n",
      "Epoch 495/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38300.6233 - val_loss: 36278.8663\n",
      "Epoch 496/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38300.3665 - val_loss: 36278.6188\n",
      "Epoch 497/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38300.1086 - val_loss: 36278.3728\n",
      "Epoch 498/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38299.8541 - val_loss: 36278.1283\n",
      "Epoch 499/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38299.5979 - val_loss: 36277.8816\n",
      "Epoch 500/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38299.3427 - val_loss: 36277.6359\n",
      "Epoch 501/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38299.0889 - val_loss: 36277.3883\n",
      "Epoch 502/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38298.8340 - val_loss: 36277.1445\n",
      "Epoch 503/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38298.5313 - val_loss: 36276.5827\n",
      "Epoch 504/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38297.4375 - val_loss: 36275.1674\n",
      "Epoch 505/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38296.7686 - val_loss: 36274.6762\n",
      "Epoch 506/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38296.3504 - val_loss: 36274.3138\n",
      "Epoch 507/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38296.0011 - val_loss: 36273.9755\n",
      "Epoch 508/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38295.6844 - val_loss: 36273.6662\n",
      "Epoch 509/1000\n",
      "825126/825126 [==============================] - 70s 84us/sample - loss: 38295.3735 - val_loss: 36273.3491\n",
      "Epoch 510/1000\n",
      "825126/825126 [==============================] - 69s 83us/sample - loss: 38295.0644 - val_loss: 36273.0475\n",
      "Epoch 511/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38294.7592 - val_loss: 36272.7501\n",
      "Epoch 512/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38294.4550 - val_loss: 36272.4442\n",
      "Epoch 513/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38294.1505 - val_loss: 36272.1493\n",
      "Epoch 514/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38294.0175 - val_loss: 36271.8988\n",
      "Epoch 515/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38293.5453 - val_loss: 36271.6014\n",
      "Epoch 516/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38293.2319 - val_loss: 36271.2443\n",
      "Epoch 517/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38292.9224 - val_loss: 36270.9502\n",
      "Epoch 518/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38292.6204 - val_loss: 36270.6629\n",
      "Epoch 519/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38292.3212 - val_loss: 36270.3632\n",
      "Epoch 520/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38292.0167 - val_loss: 36270.0685\n",
      "Epoch 521/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38291.7157 - val_loss: 36269.7648\n",
      "Epoch 522/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38291.6175 - val_loss: 36269.4537\n",
      "Epoch 523/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38291.1675 - val_loss: 36269.1510\n",
      "Epoch 524/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38290.8386 - val_loss: 36268.8624\n",
      "Epoch 525/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38290.5163 - val_loss: 36268.5710\n",
      "Epoch 526/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38290.2274 - val_loss: 36268.2933\n",
      "Epoch 527/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38289.9403 - val_loss: 36268.0292\n",
      "Epoch 528/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38289.6559 - val_loss: 36267.7506\n",
      "Epoch 529/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38289.3808 - val_loss: 36267.4864\n",
      "Epoch 530/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38289.1085 - val_loss: 36267.2240\n",
      "Epoch 531/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38288.8358 - val_loss: 36266.9641\n",
      "Epoch 532/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38288.5721 - val_loss: 36266.7083\n",
      "Epoch 533/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38288.3148 - val_loss: 36266.4565\n",
      "Epoch 534/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38288.0497 - val_loss: 36266.2217\n",
      "Epoch 535/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38287.7840 - val_loss: 36265.9531\n",
      "Epoch 536/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38287.5294 - val_loss: 36265.7324\n",
      "Epoch 537/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38287.2792 - val_loss: 36265.4859\n",
      "Epoch 538/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38287.0644 - val_loss: 36265.2657\n",
      "Epoch 539/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38286.7705 - val_loss: 36264.9668\n",
      "Epoch 540/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38286.5018 - val_loss: 36264.7156\n",
      "Epoch 541/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38286.2448 - val_loss: 36264.4688\n",
      "Epoch 542/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38285.9904 - val_loss: 36264.2222\n",
      "Epoch 543/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38285.7381 - val_loss: 36263.9807\n",
      "Epoch 544/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38285.4859 - val_loss: 36263.7412\n",
      "Epoch 545/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38285.2343 - val_loss: 36263.4954\n",
      "Epoch 546/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38284.9831 - val_loss: 36263.2538\n",
      "Epoch 547/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38284.7347 - val_loss: 36263.0111\n",
      "Epoch 548/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38284.4844 - val_loss: 36262.7705\n",
      "Epoch 549/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38284.2341 - val_loss: 36262.5296\n",
      "Epoch 550/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38283.9861 - val_loss: 36262.2904\n",
      "Epoch 551/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38283.7402 - val_loss: 36262.0498\n",
      "Epoch 552/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38283.4917 - val_loss: 36261.8143\n",
      "Epoch 553/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38283.2471 - val_loss: 36261.5721\n",
      "Epoch 554/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38282.9993 - val_loss: 36261.3336\n",
      "Epoch 555/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38282.7540 - val_loss: 36261.0978\n",
      "Epoch 556/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38282.5098 - val_loss: 36260.8601\n",
      "Epoch 557/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38282.2642 - val_loss: 36260.6227\n",
      "Epoch 558/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38282.0195 - val_loss: 36260.3887\n",
      "Epoch 559/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38281.7775 - val_loss: 36260.1526\n",
      "Epoch 560/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38281.5344 - val_loss: 36259.9166\n",
      "Epoch 561/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38281.2928 - val_loss: 36259.6804\n",
      "Epoch 562/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38281.0494 - val_loss: 36259.4472\n",
      "Epoch 563/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38280.8073 - val_loss: 36259.2110\n",
      "Epoch 564/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38280.5654 - val_loss: 36258.9799\n",
      "Epoch 565/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38280.3257 - val_loss: 36258.7467\n",
      "Epoch 566/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38280.0855 - val_loss: 36258.5146\n",
      "Epoch 567/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38279.8444 - val_loss: 36258.2825\n",
      "Epoch 568/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38279.6031 - val_loss: 36258.0467\n",
      "Epoch 569/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38279.3644 - val_loss: 36257.8165\n",
      "Epoch 570/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38279.1259 - val_loss: 36257.5841\n",
      "Epoch 571/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38278.8869 - val_loss: 36257.3498\n",
      "Epoch 572/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38278.6492 - val_loss: 36257.1280\n",
      "Epoch 573/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38278.4109 - val_loss: 36256.8924\n",
      "Epoch 574/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38278.1736 - val_loss: 36256.6669\n",
      "Epoch 575/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38277.9380 - val_loss: 36256.4315\n",
      "Epoch 576/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38277.7000 - val_loss: 36256.1988\n",
      "Epoch 577/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38277.4632 - val_loss: 36255.9719\n",
      "Epoch 578/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38277.2273 - val_loss: 36255.7468\n",
      "Epoch 579/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38276.9916 - val_loss: 36255.5135\n",
      "Epoch 580/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38276.7569 - val_loss: 36255.2860\n",
      "Epoch 581/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38276.5219 - val_loss: 36255.0563\n",
      "Epoch 582/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38276.2849 - val_loss: 36254.8296\n",
      "Epoch 583/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38276.0520 - val_loss: 36254.6021\n",
      "Epoch 584/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38275.8193 - val_loss: 36254.3769\n",
      "Epoch 585/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38275.5843 - val_loss: 36254.1491\n",
      "Epoch 586/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38275.3514 - val_loss: 36253.9252\n",
      "Epoch 587/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38275.1189 - val_loss: 36253.6947\n",
      "Epoch 588/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38274.8869 - val_loss: 36253.4704\n",
      "Epoch 589/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38274.6547 - val_loss: 36253.2469\n",
      "Epoch 590/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38274.4232 - val_loss: 36253.0232\n",
      "Epoch 591/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38274.1923 - val_loss: 36252.7957\n",
      "Epoch 592/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38273.9613 - val_loss: 36252.5715\n",
      "Epoch 593/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38273.7311 - val_loss: 36252.3435\n",
      "Epoch 594/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38273.4999 - val_loss: 36252.1217\n",
      "Epoch 595/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38273.2704 - val_loss: 36251.9032\n",
      "Epoch 596/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38273.0399 - val_loss: 36251.6750\n",
      "Epoch 597/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38272.8109 - val_loss: 36251.4518\n",
      "Epoch 598/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38272.5824 - val_loss: 36251.2286\n",
      "Epoch 599/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38272.3537 - val_loss: 36251.0071\n",
      "Epoch 600/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38272.1262 - val_loss: 36250.7848\n",
      "Epoch 601/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38271.8986 - val_loss: 36250.5616\n",
      "Epoch 602/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38271.6699 - val_loss: 36250.3419\n",
      "Epoch 603/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38271.4431 - val_loss: 36250.1230\n",
      "Epoch 604/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38271.2165 - val_loss: 36249.8992\n",
      "Epoch 605/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38270.9908 - val_loss: 36249.6822\n",
      "Epoch 606/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38270.7632 - val_loss: 36249.4595\n",
      "Epoch 607/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38270.5386 - val_loss: 36249.2379\n",
      "Epoch 608/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38270.3134 - val_loss: 36249.0186\n",
      "Epoch 609/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38270.0883 - val_loss: 36248.7992\n",
      "Epoch 610/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38269.8631 - val_loss: 36248.5804\n",
      "Epoch 611/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38269.6383 - val_loss: 36248.3629\n",
      "Epoch 612/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38269.4151 - val_loss: 36248.1422\n",
      "Epoch 613/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38269.1921 - val_loss: 36247.9231\n",
      "Epoch 614/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38268.9680 - val_loss: 36247.7109\n",
      "Epoch 615/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38268.7450 - val_loss: 36247.4892\n",
      "Epoch 616/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38268.5227 - val_loss: 36247.2746\n",
      "Epoch 617/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38268.3006 - val_loss: 36247.0562\n",
      "Epoch 618/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38268.0782 - val_loss: 36246.8375\n",
      "Epoch 619/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38267.8570 - val_loss: 36246.6237\n",
      "Epoch 620/1000\n",
      "825126/825126 [==============================] - 71s 85us/sample - loss: 38267.6343 - val_loss: 36246.4075\n",
      "Epoch 621/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38267.4140 - val_loss: 36246.1905\n",
      "Epoch 622/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38267.1933 - val_loss: 36245.9779\n",
      "Epoch 623/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38266.9726 - val_loss: 36245.7631\n",
      "Epoch 624/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38266.7555 - val_loss: 36245.5479\n",
      "Epoch 625/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38266.5342 - val_loss: 36245.3325\n",
      "Epoch 626/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38266.3167 - val_loss: 36245.1158\n",
      "Epoch 627/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38266.0963 - val_loss: 36244.9041\n",
      "Epoch 628/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38265.8787 - val_loss: 36244.6913\n",
      "Epoch 629/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38265.6608 - val_loss: 36244.4787\n",
      "Epoch 630/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38265.4421 - val_loss: 36244.2675\n",
      "Epoch 631/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38265.2258 - val_loss: 36244.0511\n",
      "Epoch 632/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38265.0093 - val_loss: 36243.8439\n",
      "Epoch 633/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38264.7923 - val_loss: 36243.6299\n",
      "Epoch 634/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38264.5751 - val_loss: 36243.4164\n",
      "Epoch 635/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38264.3597 - val_loss: 36243.2054\n",
      "Epoch 636/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38264.1439 - val_loss: 36242.9979\n",
      "Epoch 637/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38263.9280 - val_loss: 36242.7852\n",
      "Epoch 638/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38263.7143 - val_loss: 36242.5782\n",
      "Epoch 639/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38263.4995 - val_loss: 36242.3690\n",
      "Epoch 640/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38263.2847 - val_loss: 36242.1556\n",
      "Epoch 641/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38263.0712 - val_loss: 36241.9446\n",
      "Epoch 642/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38262.8584 - val_loss: 36241.7376\n",
      "Epoch 643/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38262.6461 - val_loss: 36241.5312\n",
      "Epoch 644/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38262.4323 - val_loss: 36241.3203\n",
      "Epoch 645/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38262.2204 - val_loss: 36241.1169\n",
      "Epoch 646/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38262.0104 - val_loss: 36240.9032\n",
      "Epoch 647/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38261.7972 - val_loss: 36240.7016\n",
      "Epoch 648/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38261.5868 - val_loss: 36240.4892\n",
      "Epoch 649/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38261.3744 - val_loss: 36240.2841\n",
      "Epoch 650/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38261.1632 - val_loss: 36240.0819\n",
      "Epoch 651/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38260.9536 - val_loss: 36239.8716\n",
      "Epoch 652/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38260.7437 - val_loss: 36239.6646\n",
      "Epoch 653/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38260.5352 - val_loss: 36239.4594\n",
      "Epoch 654/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38260.3252 - val_loss: 36239.2553\n",
      "Epoch 655/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38260.1182 - val_loss: 36239.0516\n",
      "Epoch 656/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38259.9100 - val_loss: 36238.8487\n",
      "Epoch 657/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38259.7001 - val_loss: 36238.6406\n",
      "Epoch 658/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38259.4914 - val_loss: 36238.4370\n",
      "Epoch 659/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38259.2834 - val_loss: 36238.2342\n",
      "Epoch 660/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38259.0778 - val_loss: 36238.0318\n",
      "Epoch 661/1000\n",
      "825126/825126 [==============================] - 75s 91us/sample - loss: 38258.8698 - val_loss: 36237.8297\n",
      "Epoch 662/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38258.6639 - val_loss: 36237.6255\n",
      "Epoch 663/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38258.4578 - val_loss: 36237.4215\n",
      "Epoch 664/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38258.2534 - val_loss: 36237.2306\n",
      "Epoch 665/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38258.0489 - val_loss: 36237.0220\n",
      "Epoch 666/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38257.8416 - val_loss: 36236.8200\n",
      "Epoch 667/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38257.6355 - val_loss: 36236.6200\n",
      "Epoch 668/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38257.4312 - val_loss: 36236.4179\n",
      "Epoch 669/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38257.2282 - val_loss: 36236.2135\n",
      "Epoch 670/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38257.0240 - val_loss: 36236.0145\n",
      "Epoch 671/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38256.8196 - val_loss: 36235.8166\n",
      "Epoch 672/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38256.6174 - val_loss: 36235.6205\n",
      "Epoch 673/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38256.4139 - val_loss: 36235.4169\n",
      "Epoch 674/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38256.2104 - val_loss: 36235.2191\n",
      "Epoch 675/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38256.0085 - val_loss: 36235.0201\n",
      "Epoch 676/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38255.8063 - val_loss: 36234.8196\n",
      "Epoch 677/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38255.6077 - val_loss: 36234.6225\n",
      "Epoch 678/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38255.4030 - val_loss: 36234.4221\n",
      "Epoch 679/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38255.2009 - val_loss: 36234.2268\n",
      "Epoch 680/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38255.0024 - val_loss: 36234.0273\n",
      "Epoch 681/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38254.8009 - val_loss: 36233.8308\n",
      "Epoch 682/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38254.6004 - val_loss: 36233.6357\n",
      "Epoch 683/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38254.4019 - val_loss: 36233.4377\n",
      "Epoch 684/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38254.2027 - val_loss: 36233.2433\n",
      "Epoch 685/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38254.0037 - val_loss: 36233.0468\n",
      "Epoch 686/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38253.8052 - val_loss: 36232.8509\n",
      "Epoch 687/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38253.6071 - val_loss: 36232.6560\n",
      "Epoch 688/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38253.4111 - val_loss: 36232.4612\n",
      "Epoch 689/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38253.2115 - val_loss: 36232.2683\n",
      "Epoch 690/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38253.0151 - val_loss: 36232.0737\n",
      "Epoch 691/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38252.8168 - val_loss: 36231.8855\n",
      "Epoch 692/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38252.6232 - val_loss: 36231.6850\n",
      "Epoch 693/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38252.4263 - val_loss: 36231.4946\n",
      "Epoch 694/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38252.2276 - val_loss: 36231.2972\n",
      "Epoch 695/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38252.0329 - val_loss: 36231.1061\n",
      "Epoch 696/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38251.8383 - val_loss: 36230.9130\n",
      "Epoch 697/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38251.6479 - val_loss: 36230.8751\n",
      "Epoch 698/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38251.4729 - val_loss: 36230.5297\n",
      "Epoch 699/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38251.2556 - val_loss: 36230.3373\n",
      "Epoch 700/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38251.0599 - val_loss: 36230.1436\n",
      "Epoch 701/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38250.8666 - val_loss: 36229.9532\n",
      "Epoch 702/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38250.6732 - val_loss: 36229.7608\n",
      "Epoch 703/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38250.4782 - val_loss: 36229.5694\n",
      "Epoch 704/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38250.2865 - val_loss: 36229.3818\n",
      "Epoch 705/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38250.0952 - val_loss: 36229.1897\n",
      "Epoch 706/1000\n",
      "825126/825126 [==============================] - 75s 91us/sample - loss: 38249.9038 - val_loss: 36229.0001\n",
      "Epoch 707/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38249.7126 - val_loss: 36228.8129\n",
      "Epoch 708/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38249.5205 - val_loss: 36228.6195\n",
      "Epoch 709/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38249.3290 - val_loss: 36228.4317\n",
      "Epoch 710/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38249.1385 - val_loss: 36228.2425\n",
      "Epoch 711/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38248.9464 - val_loss: 36228.0539\n",
      "Epoch 712/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38248.7576 - val_loss: 36227.8663\n",
      "Epoch 713/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38248.5672 - val_loss: 36227.6801\n",
      "Epoch 714/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38248.3764 - val_loss: 36227.4950\n",
      "Epoch 715/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38248.1860 - val_loss: 36227.3044\n",
      "Epoch 716/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38247.9971 - val_loss: 36227.1118\n",
      "Epoch 717/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38247.8069 - val_loss: 36226.9258\n",
      "Epoch 718/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38247.7788 - val_loss: 36226.8530\n",
      "Epoch 719/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38247.4745 - val_loss: 36226.5767\n",
      "Epoch 720/1000\n",
      "825126/825126 [==============================] - 75s 90us/sample - loss: 38247.2636 - val_loss: 36226.3846\n",
      "Epoch 721/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38247.0705 - val_loss: 36226.1929\n",
      "Epoch 722/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38246.8795 - val_loss: 36226.0047\n",
      "Epoch 723/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38246.6903 - val_loss: 36225.8162\n",
      "Epoch 724/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38246.5018 - val_loss: 36225.6294\n",
      "Epoch 725/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38246.3138 - val_loss: 36225.4411\n",
      "Epoch 726/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38246.1243 - val_loss: 36225.2558\n",
      "Epoch 727/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38245.9376 - val_loss: 36225.0719\n",
      "Epoch 728/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38245.7506 - val_loss: 36224.8820\n",
      "Epoch 729/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38245.5633 - val_loss: 36224.6959\n",
      "Epoch 730/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38245.3774 - val_loss: 36224.5114\n",
      "Epoch 731/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38245.1912 - val_loss: 36224.3256\n",
      "Epoch 732/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38245.0042 - val_loss: 36224.1408\n",
      "Epoch 733/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38244.8173 - val_loss: 36223.9550\n",
      "Epoch 734/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38244.6320 - val_loss: 36223.7674\n",
      "Epoch 735/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38244.4438 - val_loss: 36223.5800\n",
      "Epoch 736/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38244.2569 - val_loss: 36223.3955\n",
      "Epoch 737/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38244.0700 - val_loss: 36223.2092\n",
      "Epoch 738/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38243.8827 - val_loss: 36223.0229\n",
      "Epoch 739/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38243.6971 - val_loss: 36222.8373\n",
      "Epoch 740/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38243.5116 - val_loss: 36222.6529\n",
      "Epoch 741/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38243.3271 - val_loss: 36222.4686\n",
      "Epoch 742/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38243.1427 - val_loss: 36222.2846\n",
      "Epoch 743/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38242.9582 - val_loss: 36222.0993\n",
      "Epoch 744/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38242.7716 - val_loss: 36221.9145\n",
      "Epoch 745/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38242.5871 - val_loss: 36221.7300\n",
      "Epoch 746/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38242.4020 - val_loss: 36221.5484\n",
      "Epoch 747/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38242.2198 - val_loss: 36221.3632\n",
      "Epoch 748/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38242.0338 - val_loss: 36221.1777\n",
      "Epoch 749/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38241.8501 - val_loss: 36220.9949\n",
      "Epoch 750/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38241.6657 - val_loss: 36220.8115\n",
      "Epoch 751/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38241.4814 - val_loss: 36220.6266\n",
      "Epoch 752/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38241.2992 - val_loss: 36220.4443\n",
      "Epoch 753/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38241.1153 - val_loss: 36220.2639\n",
      "Epoch 754/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38240.9438 - val_loss: 36220.0945\n",
      "Epoch 755/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38240.7498 - val_loss: 36219.8937\n",
      "Epoch 756/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38240.5636 - val_loss: 36219.7099\n",
      "Epoch 757/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38240.3797 - val_loss: 36219.5253\n",
      "Epoch 758/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38240.1978 - val_loss: 36219.3456\n",
      "Epoch 759/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38240.0133 - val_loss: 36219.1610\n",
      "Epoch 760/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38239.8297 - val_loss: 36218.9735\n",
      "Epoch 761/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38239.6462 - val_loss: 36218.7966\n",
      "Epoch 762/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38239.4647 - val_loss: 36218.6118\n",
      "Epoch 763/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38239.2799 - val_loss: 36218.4287\n",
      "Epoch 764/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38239.0972 - val_loss: 36218.2481\n",
      "Epoch 765/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38238.9150 - val_loss: 36218.0608\n",
      "Epoch 766/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38238.7318 - val_loss: 36217.8777\n",
      "Epoch 767/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38238.5477 - val_loss: 36217.6955\n",
      "Epoch 768/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38238.3664 - val_loss: 36217.5121\n",
      "Epoch 769/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38238.1834 - val_loss: 36217.3303\n",
      "Epoch 770/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38237.9979 - val_loss: 36217.1451\n",
      "Epoch 771/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38237.8165 - val_loss: 36216.9636\n",
      "Epoch 772/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38237.6337 - val_loss: 36216.7807\n",
      "Epoch 773/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38237.4520 - val_loss: 36216.5950\n",
      "Epoch 774/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38237.2681 - val_loss: 36216.4133\n",
      "Epoch 775/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38237.0849 - val_loss: 36216.2311\n",
      "Epoch 776/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38236.9012 - val_loss: 36216.0482\n",
      "Epoch 777/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38236.7192 - val_loss: 36215.8650\n",
      "Epoch 778/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38236.5358 - val_loss: 36215.6813\n",
      "Epoch 779/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38236.3540 - val_loss: 36215.5009\n",
      "Epoch 780/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38236.1713 - val_loss: 36215.3178\n",
      "Epoch 781/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38235.9878 - val_loss: 36215.1320\n",
      "Epoch 782/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38235.8043 - val_loss: 36214.9497\n",
      "Epoch 783/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38235.6216 - val_loss: 36214.7661\n",
      "Epoch 784/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38235.4398 - val_loss: 36214.5812\n",
      "Epoch 785/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38235.2554 - val_loss: 36214.4004\n",
      "Epoch 786/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38235.0740 - val_loss: 36214.2153\n",
      "Epoch 787/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38234.8904 - val_loss: 36214.0335\n",
      "Epoch 788/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38234.7077 - val_loss: 36213.8507\n",
      "Epoch 789/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38234.5241 - val_loss: 36213.6678\n",
      "Epoch 790/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38234.3417 - val_loss: 36213.4862\n",
      "Epoch 791/1000\n",
      "825126/825126 [==============================] - 75s 90us/sample - loss: 38234.1601 - val_loss: 36213.3018\n",
      "Epoch 792/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38233.9764 - val_loss: 36213.1175\n",
      "Epoch 793/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38233.7927 - val_loss: 36212.9327\n",
      "Epoch 794/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38233.6100 - val_loss: 36212.7493\n",
      "Epoch 795/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38233.4271 - val_loss: 36212.5670\n",
      "Epoch 796/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38233.2450 - val_loss: 36212.3863\n",
      "Epoch 797/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38233.0612 - val_loss: 36212.2037\n",
      "Epoch 798/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38232.8792 - val_loss: 36212.0216\n",
      "Epoch 799/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38232.6969 - val_loss: 36211.8356\n",
      "Epoch 800/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38232.5134 - val_loss: 36211.6526\n",
      "Epoch 801/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38232.3310 - val_loss: 36211.4707\n",
      "Epoch 802/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38232.1495 - val_loss: 36211.2868\n",
      "Epoch 803/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38231.9658 - val_loss: 36211.1038\n",
      "Epoch 804/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38231.7823 - val_loss: 36210.9201\n",
      "Epoch 805/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38231.6007 - val_loss: 36210.7368\n",
      "Epoch 806/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38231.4176 - val_loss: 36210.5531\n",
      "Epoch 807/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38231.2347 - val_loss: 36210.3700\n",
      "Epoch 808/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38231.0513 - val_loss: 36210.1897\n",
      "Epoch 809/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38230.8680 - val_loss: 36210.0043\n",
      "Epoch 810/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38230.6860 - val_loss: 36209.8245\n",
      "Epoch 811/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38230.5030 - val_loss: 36209.6403\n",
      "Epoch 812/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38230.3206 - val_loss: 36209.4582\n",
      "Epoch 813/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38230.1382 - val_loss: 36209.2765\n",
      "Epoch 814/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38229.9558 - val_loss: 36209.0913\n",
      "Epoch 815/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38229.7724 - val_loss: 36208.9150\n",
      "Epoch 816/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38229.5908 - val_loss: 36208.7237\n",
      "Epoch 817/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38229.4075 - val_loss: 36208.5392\n",
      "Epoch 818/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38229.2243 - val_loss: 36208.3604\n",
      "Epoch 819/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38229.0414 - val_loss: 36208.1748\n",
      "Epoch 820/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38228.8581 - val_loss: 36207.9958\n",
      "Epoch 821/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38228.6767 - val_loss: 36207.8084\n",
      "Epoch 822/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38228.4938 - val_loss: 36207.6257\n",
      "Epoch 823/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38228.3117 - val_loss: 36207.4413\n",
      "Epoch 824/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38228.1284 - val_loss: 36207.2585\n",
      "Epoch 825/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38227.9458 - val_loss: 36207.0754\n",
      "Epoch 826/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38227.7634 - val_loss: 36206.8970\n",
      "Epoch 827/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38227.5798 - val_loss: 36206.7101\n",
      "Epoch 828/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38227.3972 - val_loss: 36206.5281\n",
      "Epoch 829/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38227.2138 - val_loss: 36206.3426\n",
      "Epoch 830/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38227.0363 - val_loss: 36206.1644\n",
      "Epoch 831/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38226.8506 - val_loss: 36205.9778\n",
      "Epoch 832/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38226.6677 - val_loss: 36205.7964\n",
      "Epoch 833/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38226.4859 - val_loss: 36205.6158\n",
      "Epoch 834/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38226.3036 - val_loss: 36205.4318\n",
      "Epoch 835/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38226.1190 - val_loss: 36205.2431\n",
      "Epoch 836/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38225.9368 - val_loss: 36205.0609\n",
      "Epoch 837/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38225.7542 - val_loss: 36204.8838\n",
      "Epoch 838/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38225.5722 - val_loss: 36204.6961\n",
      "Epoch 839/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38225.3889 - val_loss: 36204.5130\n",
      "Epoch 840/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38225.2071 - val_loss: 36204.3294\n",
      "Epoch 841/1000\n",
      "825126/825126 [==============================] - 75s 90us/sample - loss: 38225.0230 - val_loss: 36204.1483\n",
      "Epoch 842/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38224.8405 - val_loss: 36203.9632\n",
      "Epoch 843/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38224.6580 - val_loss: 36203.7814\n",
      "Epoch 844/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38224.4764 - val_loss: 36203.5996\n",
      "Epoch 845/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38224.2926 - val_loss: 36203.4193\n",
      "Epoch 846/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38224.1105 - val_loss: 36203.2318\n",
      "Epoch 847/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38223.9278 - val_loss: 36203.0482\n",
      "Epoch 848/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38223.7465 - val_loss: 36202.8673\n",
      "Epoch 849/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38223.5635 - val_loss: 36202.6828\n",
      "Epoch 850/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38223.3803 - val_loss: 36202.4998\n",
      "Epoch 851/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38223.1971 - val_loss: 36202.3165\n",
      "Epoch 852/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38223.0153 - val_loss: 36202.1340\n",
      "Epoch 853/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38222.8319 - val_loss: 36201.9495\n",
      "Epoch 854/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38222.6505 - val_loss: 36201.7668\n",
      "Epoch 855/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38222.4662 - val_loss: 36201.5845\n",
      "Epoch 856/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38222.2850 - val_loss: 36201.4001\n",
      "Epoch 857/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38222.1021 - val_loss: 36201.2171\n",
      "Epoch 858/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38221.9192 - val_loss: 36201.0316\n",
      "Epoch 859/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38221.7368 - val_loss: 36200.8510\n",
      "Epoch 860/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38221.5539 - val_loss: 36200.6693\n",
      "Epoch 861/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38221.3707 - val_loss: 36200.4828\n",
      "Epoch 862/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38221.1877 - val_loss: 36200.3009\n",
      "Epoch 863/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38221.0048 - val_loss: 36200.1166\n",
      "Epoch 864/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38220.8233 - val_loss: 36199.9358\n",
      "Epoch 865/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38220.6459 - val_loss: 36199.7534\n",
      "Epoch 866/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38220.4577 - val_loss: 36199.5715\n",
      "Epoch 867/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38220.2753 - val_loss: 36199.3910\n",
      "Epoch 868/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38220.0926 - val_loss: 36199.2033\n",
      "Epoch 869/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38219.9100 - val_loss: 36199.0192\n",
      "Epoch 870/1000\n",
      "825126/825126 [==============================] - 74s 90us/sample - loss: 38219.7271 - val_loss: 36198.8357\n",
      "Epoch 871/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38219.5455 - val_loss: 36198.6537\n",
      "Epoch 872/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38219.3618 - val_loss: 36198.4720\n",
      "Epoch 873/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38219.1793 - val_loss: 36198.2870\n",
      "Epoch 874/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38218.9966 - val_loss: 36198.1038\n",
      "Epoch 875/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38218.8142 - val_loss: 36197.9219\n",
      "Epoch 876/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38218.6355 - val_loss: 36197.7428\n",
      "Epoch 877/1000\n",
      "825126/825126 [==============================] - 75s 91us/sample - loss: 38218.4494 - val_loss: 36197.5554\n",
      "Epoch 878/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38218.2673 - val_loss: 36197.3724\n",
      "Epoch 879/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38218.0836 - val_loss: 36197.1915\n",
      "Epoch 880/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38217.9020 - val_loss: 36197.0077\n",
      "Epoch 881/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38217.7181 - val_loss: 36196.8227\n",
      "Epoch 882/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38217.5356 - val_loss: 36196.6412\n",
      "Epoch 883/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38217.3544 - val_loss: 36196.4563\n",
      "Epoch 884/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38217.1722 - val_loss: 36196.2735\n",
      "Epoch 885/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38216.9895 - val_loss: 36196.0912\n",
      "Epoch 886/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38216.8082 - val_loss: 36195.9127\n",
      "Epoch 887/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38216.6251 - val_loss: 36195.7259\n",
      "Epoch 888/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38216.4433 - val_loss: 36195.5420\n",
      "Epoch 889/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38216.2593 - val_loss: 36195.3587\n",
      "Epoch 890/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38216.0777 - val_loss: 36195.1800\n",
      "Epoch 891/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38215.8971 - val_loss: 36194.9948\n",
      "Epoch 892/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38215.7140 - val_loss: 36194.8095\n",
      "Epoch 893/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38215.5308 - val_loss: 36194.6299\n",
      "Epoch 894/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38215.3479 - val_loss: 36194.4478\n",
      "Epoch 895/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38215.1650 - val_loss: 36194.2631\n",
      "Epoch 896/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38214.9831 - val_loss: 36194.0793\n",
      "Epoch 897/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38214.8007 - val_loss: 36193.8964\n",
      "Epoch 898/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38214.6191 - val_loss: 36193.7160\n",
      "Epoch 899/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38214.4362 - val_loss: 36193.5308\n",
      "Epoch 900/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38214.2531 - val_loss: 36193.3501\n",
      "Epoch 901/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38214.0713 - val_loss: 36193.1669\n",
      "Epoch 902/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38213.8896 - val_loss: 36192.9849\n",
      "Epoch 903/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38213.7076 - val_loss: 36192.8004\n",
      "Epoch 904/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38213.5245 - val_loss: 36192.6155\n",
      "Epoch 905/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38213.3412 - val_loss: 36192.4329\n",
      "Epoch 906/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38213.1598 - val_loss: 36192.2561\n",
      "Epoch 907/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38212.9768 - val_loss: 36192.0720\n",
      "Epoch 908/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38212.7950 - val_loss: 36191.8859\n",
      "Epoch 909/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38212.6119 - val_loss: 36191.7024\n",
      "Epoch 910/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38212.4301 - val_loss: 36191.5201\n",
      "Epoch 911/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38212.2471 - val_loss: 36191.3372\n",
      "Epoch 912/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38212.0663 - val_loss: 36191.1547\n",
      "Epoch 913/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38211.8852 - val_loss: 36190.9712\n",
      "Epoch 914/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38211.7000 - val_loss: 36190.7881\n",
      "Epoch 915/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38211.5179 - val_loss: 36190.6044\n",
      "Epoch 916/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38211.3375 - val_loss: 36190.4224\n",
      "Epoch 917/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38211.1535 - val_loss: 36190.2426\n",
      "Epoch 918/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38210.9713 - val_loss: 36190.0583\n",
      "Epoch 919/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38210.7895 - val_loss: 36189.8763\n",
      "Epoch 920/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38210.6059 - val_loss: 36189.6914\n",
      "Epoch 921/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38210.4244 - val_loss: 36189.5098\n",
      "Epoch 922/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38210.2422 - val_loss: 36189.3244\n",
      "Epoch 923/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38210.0587 - val_loss: 36189.1408\n",
      "Epoch 924/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38209.8769 - val_loss: 36188.9602\n",
      "Epoch 925/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38209.6945 - val_loss: 36188.7729\n",
      "Epoch 926/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38209.5120 - val_loss: 36188.5939\n",
      "Epoch 927/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38209.3291 - val_loss: 36188.4099\n",
      "Epoch 928/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38209.1470 - val_loss: 36188.2281\n",
      "Epoch 929/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38208.9646 - val_loss: 36188.0436\n",
      "Epoch 930/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38208.7818 - val_loss: 36187.8605\n",
      "Epoch 931/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38208.5998 - val_loss: 36187.6795\n",
      "Epoch 932/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38208.4174 - val_loss: 36187.4958\n",
      "Epoch 933/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38208.2350 - val_loss: 36187.3150\n",
      "Epoch 934/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38208.0545 - val_loss: 36187.1294\n",
      "Epoch 935/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38207.8709 - val_loss: 36186.9489\n",
      "Epoch 936/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38207.6869 - val_loss: 36186.7650\n",
      "Epoch 937/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38207.5058 - val_loss: 36186.5836\n",
      "Epoch 938/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38207.3238 - val_loss: 36186.3992\n",
      "Epoch 939/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38207.1404 - val_loss: 36186.2173\n",
      "Epoch 940/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38206.9586 - val_loss: 36186.0320\n",
      "Epoch 941/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38206.7774 - val_loss: 36185.8716\n",
      "Epoch 942/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38206.5969 - val_loss: 36185.6661\n",
      "Epoch 943/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38206.4125 - val_loss: 36185.4844\n",
      "Epoch 944/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38206.2304 - val_loss: 36185.2991\n",
      "Epoch 945/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38206.0467 - val_loss: 36185.1204\n",
      "Epoch 946/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38205.8653 - val_loss: 36184.9342\n",
      "Epoch 947/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38205.6828 - val_loss: 36184.7518\n",
      "Epoch 948/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38205.5003 - val_loss: 36184.5713\n",
      "Epoch 949/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38205.3200 - val_loss: 36184.3859\n",
      "Epoch 950/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38205.1353 - val_loss: 36184.2040\n",
      "Epoch 951/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38204.9529 - val_loss: 36184.0209\n",
      "Epoch 952/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38204.7700 - val_loss: 36183.8383\n",
      "Epoch 953/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38204.5880 - val_loss: 36183.6547\n",
      "Epoch 954/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38204.4059 - val_loss: 36183.4721\n",
      "Epoch 955/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38204.2241 - val_loss: 36183.2878\n",
      "Epoch 956/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38204.0436 - val_loss: 36183.1068\n",
      "Epoch 957/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38203.8599 - val_loss: 36182.9245\n",
      "Epoch 958/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38203.6769 - val_loss: 36182.7399\n",
      "Epoch 959/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38203.4945 - val_loss: 36182.5581\n",
      "Epoch 960/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38203.3126 - val_loss: 36182.3749\n",
      "Epoch 961/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38203.1297 - val_loss: 36182.1918\n",
      "Epoch 962/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38202.9477 - val_loss: 36182.0100\n",
      "Epoch 963/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38202.7654 - val_loss: 36181.8280\n",
      "Epoch 964/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38202.5832 - val_loss: 36181.6434\n",
      "Epoch 965/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38202.4007 - val_loss: 36181.4600\n",
      "Epoch 966/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38202.2192 - val_loss: 36181.2809\n",
      "Epoch 967/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38202.0369 - val_loss: 36181.0977\n",
      "Epoch 968/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38201.8555 - val_loss: 36180.9119\n",
      "Epoch 969/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38201.6718 - val_loss: 36180.7294\n",
      "Epoch 970/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38201.4897 - val_loss: 36180.5463\n",
      "Epoch 971/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38201.3084 - val_loss: 36180.3650\n",
      "Epoch 972/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38201.1241 - val_loss: 36180.1821\n",
      "Epoch 973/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38200.9465 - val_loss: 36179.9978\n",
      "Epoch 974/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38200.7616 - val_loss: 36179.8170\n",
      "Epoch 975/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38200.5774 - val_loss: 36179.6340\n",
      "Epoch 976/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38200.3949 - val_loss: 36179.4520\n",
      "Epoch 977/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38200.2136 - val_loss: 36179.2673\n",
      "Epoch 978/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38200.0322 - val_loss: 36179.0845\n",
      "Epoch 979/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38199.8499 - val_loss: 36178.8996\n",
      "Epoch 980/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38199.6663 - val_loss: 36178.7171\n",
      "Epoch 981/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38199.4841 - val_loss: 36178.5348\n",
      "Epoch 982/1000\n",
      "825126/825126 [==============================] - 73s 88us/sample - loss: 38199.3030 - val_loss: 36178.3526\n",
      "Epoch 983/1000\n",
      "825126/825126 [==============================] - 74s 89us/sample - loss: 38199.1202 - val_loss: 36178.1727\n",
      "Epoch 984/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38198.9382 - val_loss: 36177.9855\n",
      "Epoch 985/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38198.7575 - val_loss: 36177.8120\n",
      "Epoch 986/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38198.5754 - val_loss: 36177.6232\n",
      "Epoch 987/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38198.3930 - val_loss: 36177.4399\n",
      "Epoch 988/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38198.2094 - val_loss: 36177.2564\n",
      "Epoch 989/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38198.0270 - val_loss: 36177.0768\n",
      "Epoch 990/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38197.8442 - val_loss: 36176.8905\n",
      "Epoch 991/1000\n",
      "825126/825126 [==============================] - 71s 87us/sample - loss: 38197.6611 - val_loss: 36176.7096\n",
      "Epoch 992/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38197.4797 - val_loss: 36176.5254\n",
      "Epoch 993/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38197.2967 - val_loss: 36176.3401\n",
      "Epoch 994/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38197.1146 - val_loss: 36176.1608\n",
      "Epoch 995/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38196.9335 - val_loss: 36175.9749\n",
      "Epoch 996/1000\n",
      "825126/825126 [==============================] - 70s 85us/sample - loss: 38196.7517 - val_loss: 36175.7917\n",
      "Epoch 997/1000\n",
      "825126/825126 [==============================] - 72s 87us/sample - loss: 38196.5687 - val_loss: 36175.6106\n",
      "Epoch 998/1000\n",
      "825126/825126 [==============================] - 71s 86us/sample - loss: 38196.3869 - val_loss: 36175.4294\n",
      "Epoch 999/1000\n",
      "825126/825126 [==============================] - 72s 88us/sample - loss: 38196.2042 - val_loss: 36175.2455\n",
      "Epoch 1000/1000\n",
      "825126/825126 [==============================] - 73s 89us/sample - loss: 38196.0224 - val_loss: 36175.0615\n"
     ]
    }
   ],
   "source": [
    "logs = m.fit(input_pts,target_pts,batch_size=10000,validation_split=0.25,epochs=1000,\n",
    "             callbacks=[EarlyStoppingAtMinLoss()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "israeli-shepherd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3995951,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['nearest_pts'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
